{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1717847643531,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"gokGmHeEWd3h","outputId":"f219b1fe-b5ac-4438-bb35-d3be6338c1bf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","from sklearn.metrics import roc_curve, roc_auc_score\n","from keras.callbacks import EarlyStopping\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras import Input\n","from keras.optimizers import AdamW\n","import shap\n","tf.random.set_seed(2)\n","\n","import pyhf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create output directory\n","OUT_DIR = \"./MachineLearning/\"\n","os.makedirs(OUT_DIR, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2562,"status":"ok","timestamp":1717847649047,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"NDKdxL2OWd3k"},"outputs":[],"source":["# Load the dataframe with MC data and their labels (signal = 1, background = 0)\n","dataframe = pd.read_csv('dataframe.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1717847649048,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"lUDd005QWd3k","outputId":"7a3ce8d8-5e5a-48dc-97ea-609b769fe45d"},"outputs":[],"source":["# Create train weights such that the sum of train weights for signal and background is equal (ratio bkg/sig = 1:1)\n","dataframe['train_weight'] = 1.\n","dataframe.loc[dataframe.query('label==1').index,'train_weight'] = (dataframe.loc[dataframe.query('label==1').index,'scaleweight']) / dataframe.loc[dataframe.query('label==1').index,'scaleweight'].sum()\n","dataframe.loc[dataframe.query('label==0').index,'train_weight'] = dataframe.loc[dataframe.query('label==0').index,'scaleweight'] / dataframe.loc[dataframe.query('label==0').index,'scaleweight'].sum()\n","\n","sum_w_sig = dataframe.query('label==0')['train_weight'].sum()\n","sum_w_bkg = dataframe.query('label==1')['train_weight'].sum()\n","print(f'Sum of weights for training Signal {sum_w_sig:.3} and Background {sum_w_bkg:.3}')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":222,"status":"ok","timestamp":1717847649048,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"_W8MaOerWd3k"},"outputs":[],"source":["# Define the features and weights to be used for training\n","train_features = ['etmiss', 'mtw', 'leadleptPt', 'leadleptEta',\n","       'leadleptE', 'leadleptPhi', 'Q_leadlep', 'n_TopLRjets',\n","       'leadTopLRjet_pt', 'leadTopLRjet_eta', 'leadTopLRjet_phi',\n","       'leadTopLRjet_m', 'leadTopLRjet_Tau32', 'n_jets', 'leadjet_pt',\n","       'leadjet_eta', 'n_bjets', 'leadbjet_pt', 'leadbjet_eta', 'ttbarMLR']\n","weights = [\"scaleweight\", \"train_weight\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":198,"status":"ok","timestamp":1717847649048,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"cE8LHZVOWd3l"},"outputs":[],"source":["# Split the dataframe into train, validation and test samples, and scale the features\n","x_train, x_val, y_train, y_val, w_train_full, w_val_full = train_test_split(dataframe[train_features].values, dataframe['label'].values, dataframe[weights].values, train_size = 1/3, random_state = 64)\n","x_val, x_test, y_val, y_test, w_val_full, w_test_full = train_test_split(x_val, y_val, w_val_full, test_size=1/2, random_state = 64)\n","\n","scaler = StandardScaler()\n","scaler.fit(x_train)\n","\n","x_train = scaler.transform(x_train)\n","x_val = scaler.transform(x_val)\n","x_test = scaler.transform(x_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Verify that the ratio of signal to background in the training, validation and test sets is equal to 1\n","print(f\"Ratio of signal to background in training set: {w_train_full[:, 1][y_train==1].sum() / w_train_full[:, 1][y_train==0].sum():.2f}\")\n","print(f\"Ratio of signal to background in validation set: {w_val_full[:, 1][y_val==1].sum() / w_val_full[:, 1][y_val==0].sum():.2f}\")\n","print(f\"Ratio of signal to background in test set: {w_test_full[:, 1][y_test==1].sum() / w_test_full[:, 1][y_test==0].sum():.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":171,"status":"ok","timestamp":1717847649048,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"cpdDHJr8Wd3l"},"outputs":[],"source":["# Separate the weights into scaleweights to use in the fit, and train weights to use in the training\n","def weight_separation(w):\n","    scaleweights = w[:, 0]\n","    train = w[:, 1]\n","    return scaleweights, train\n","\n","\"\"\"\n","w_<set>_scale are the scaleweights to be used in the fit\n","w_<set> are the train weights to be used in the training\n","\"\"\"\n","w_train_scale, w_train = weight_separation(w_train_full)\n","w_val_scale, w_val = weight_separation(w_val_full)\n","w_test_scale, w_test = weight_separation(w_test_full)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141,"status":"ok","timestamp":1717847649048,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"TeHdJKJyWd3l","outputId":"01b637de-107b-469f-e658-ea3fcb6e3e22"},"outputs":[],"source":["\"\"\"\n","Define the model\n","----------------\n","The model has 5 layers:\n","- Input layer\n","- 3 Hidden layers\n","- Output layer\n","\n","The input layer has the same number of nodes as the number of features in the training set.\n","The hidden layers have 75, 125, 125, 75 nodes respectively. The first hidden layer has dropout of 0.1.\n","The output layer has 1 node.\n","\n","The activation functions for the hidden layers are 'softsign' and 'relu'.\n","The activation function for the output layer is 'sigmoid'.\n","\n","The loss function is 'binary_crossentropy' and the optimizer is 'AdamW'.\n","The learning rate is 1e-3 and the weight decay is 1e-1.\n","\"\"\"\n","\n","model = Sequential([Input(shape=(x_train.shape[1],))])\n","\n","# Input and Hidden layers\n","model.add(Dense(75, activation='softsign'))\n","model.add(Dropout(0.1))\n","model.add(Dense(125, activation='softsign'))\n","model.add(Dense(125, activation='relu'))\n","model.add(Dense(75, activation='relu'))\n","\n","# Output layer\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer=AdamW(learning_rate=1e-3, weight_decay=1e-1), weighted_metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111048,"status":"ok","timestamp":1717847760082,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"wc6Ig0KDWd3l","outputId":"fb7add14-368a-4634-fc81-0de8592b9e03"},"outputs":[],"source":["# Train the model\n","history = model.fit(x_train, y_train, sample_weight=w_train, validation_data=(x_val, y_val, w_val), epochs=100, batch_size=1024, callbacks=[EarlyStopping(patience=2)])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":927},"executionInfo":{"elapsed":32340,"status":"ok","timestamp":1717847792390,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"_HoGJCxxWd3l","outputId":"d4eca85f-8756-4d6a-a708-075032599463"},"outputs":[],"source":["# Plot the Loss per epoch \n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='validation')\n","plt.title('Model loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"loss.png\")\n","plt.show()\n","\n","# Plot the Accuracy per epoch\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='validation')\n","plt.title('Model accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"accuracy.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot loss and accuracy in the same plot but with different scales left and right\n","fig, ax1 = plt.subplots()\n","\n","color = 'tab:red'\n","ax1.set_xlabel('Epoch')\n","ax1.set_ylabel('Loss', color=color)\n","ax1.plot(history.history['loss'], color=\"red\", label='train')\n","ax1.plot(history.history['val_loss'], color=color, label='validation')\n","ax1.tick_params(axis='y', labelcolor=color)\n","ax1.legend(loc='upper left')\n","\n","ax2 = ax1.twinx()\n","color = 'tab:blue'\n","ax2.set_ylabel('Accuracy', color=color)\n","ax2.plot(history.history['accuracy'], color=\"blue\", label='train')\n","ax2.plot(history.history['val_accuracy'], color=color, label='validation')\n","ax2.tick_params(axis='y', labelcolor=color)\n","ax2.legend(loc='upper right')\n","\n","fig.tight_layout()\n","plt.title('Model loss and accuracy')\n","plt.savefig(OUT_DIR + \"loss_accuracy.png\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21958,"status":"ok","timestamp":1717847814321,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"MTAhm4nvWd3m","outputId":"f38797f8-79d7-4c80-89fa-fb732fb10563"},"outputs":[],"source":["# Compute the predictions for the training, validation and test sets\n","y_train_pred = model.predict(x_train)\n","y_val_pred = model.predict(x_val)\n","y_test_pred = model.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1717847814323,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"YR_BmI9MWd3m","outputId":"b1b2c63d-bb1e-485c-af9b-7100b4ffab50"},"outputs":[],"source":["# Plot the histogram of the NN output for the test set\n","bin = plt.hist(y_test_pred[y_test==0], bins=20, density=False, histtype='step', color='blue', label='Background', weights=w_test_scale[y_test==0])\n","plt.hist(y_test_pred[y_test==1], bins=bin[1], density=False, histtype='step', color='red', label='Signal', weights=w_test_scale[y_test==1])\n","\n","plt.xlabel('NN output')\n","plt.ylabel('Counts')\n","plt.title('NN output for test set')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_test_output.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"elapsed":1308,"status":"ok","timestamp":1717847815543,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"GIYUbZs5Wd3m","outputId":"1ce12daf-6b2c-4fc2-da12-b31674873bf9"},"outputs":[],"source":["# Plot the histogram of the NN output for the training and validation sets\n","bins = plt.hist(y_train_pred[y_train==1], bins=20, density=False, histtype='step', color='blue', label='Signal (train)', weights=w_train[y_train==1])\n","plt.hist(y_val_pred[y_val==1], bins=bins[1], density=False, histtype='step', color='red', label='Signal (val)', weights=w_val[y_val==1])\n","bins = plt.hist(y_train_pred[y_train==0], bins=bins[1], density=False, histtype='step', color='green', label='Background (train)', weights=w_train[y_train==0])\n","plt.hist(y_val_pred[y_val==0], bins=bins[1], density=False, histtype='step', color='orange', label='Background (val)', weights=w_val[y_val==0])\n","\n","plt.xlabel('NN output')\n","plt.ylabel('Counts')\n","plt.title('NN output for training and validation sets')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_train_val_output.png\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":462,"status":"ok","timestamp":1717847815543,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"Gp0HHteDWd3m","outputId":"d40926c0-85e2-40c7-a5cb-80f148468a82"},"outputs":[],"source":["# ROC curve for the test set\n","fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n","auc = roc_auc_score(y_test, y_test_pred)\n","plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n","plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n","plt.title('ROC curve')\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"ROC_curve.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":435,"status":"ok","timestamp":1717847815543,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"9ZwMB2ofWd3m"},"outputs":[],"source":["# Create dataframe of the predictions and its respective scaleweights\n","def join_y_w(y, w):\n","    df = pd.DataFrame({'y_pred': y, 'scaleweights': w})\n","    return df\n","\n","df_train = join_y_w(y_train_pred[:, 0], w_train_scale[:])\n","df_val = join_y_w(y_val_pred[:, 0], w_val_scale[:])\n","df_test = join_y_w(y_test_pred[:, 0], w_test_scale[:]*3) # Multiply by 3 to account for the 1/3 of the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1717847815543,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"DHEODbliWd3n","outputId":"c02901a0-fac2-4f13-dff9-e6fa162502c7"},"outputs":[],"source":["# Import the measured data\n","data = pd.read_csv('Output_ZPrimeBoostedAnalysis/data.csv', delimiter= \" \")\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":334,"status":"ok","timestamp":1717847815543,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"bP_HpD7ZWd3n"},"outputs":[],"source":["# Apply the same scaling to the measured data and get the weights\n","x_data = scaler.transform(data[train_features].values)\n","w_data = data[\"weight\"].values\n","\n","# Compute the predictions for the measured data\n","y_data_pred = model.predict(x_data)\n","\n","# Create dataframe of the predictions and its respective weights\n","df_data = join_y_w(y_data_pred[:, 0], w_data[:])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1717847816178,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"NpgS8vxoWd3n","outputId":"02ca9a79-d937-4556-e297-f35fdbe552b1"},"outputs":[],"source":["# Plot the histogram of the NN output for the measured data and the test set\n","bins = plt.hist(df_data['y_pred'], bins=20, histtype='step', color='blue', label='Measured Data', weights=df_data['scaleweights'])\n","plt.hist(df_test['y_pred'], bins=bins[1], histtype='step', color='red', label='Test Set Output', weights=df_test['scaleweights'])\n","plt.xlabel('NN output')\n","plt.ylabel('Events')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_data_test_output.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":100,"status":"ok","timestamp":1717847816178,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"yIYw53MsWd3o"},"outputs":[],"source":["# Create the distribution of the measured data and the test set signal and background NN output\n","N, bins = np.histogram(df_data['y_pred'], bins=20, weights=df_data['scaleweights'])\n","\n","B = np.histogram(df_test['y_pred'][y_test==0], bins=bins, weights=df_test['scaleweights'][y_test==0])[0]\n","S = np.histogram(df_test['y_pred'][y_test==1], bins=bins, weights=df_test['scaleweights'][y_test==1])[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Select the bins where there is less signal (the background is more pure) to use in the fit to discover the background normalization factor epsilon\n","N2 = N[:9]\n","B2 = B[:9]\n","S2 = S[:9]\n","\n","#plot\n","plt.hist(bins[:9], bins, weights=N2, histtype='step', color='blue', label='Measured Data')\n","plt.hist(bins[:9], bins, weights=B2, histtype='step', color='red', label='Background')\n","plt.hist(bins[:9], bins, weights=S2, histtype='step', color='green', label='Signal')\n","plt.xlabel('NN output')\n","plt.ylabel('Events')\n","plt.title('Measured Data and Signal and Background NN output (test set)')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_bkg_cut.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the NN output and its respective type and weights to a csv file (measured data, test set, train set, validation set)\n","dataframe_to_save1 = pd.DataFrame({'Type': 'Observed', 'NN output': df_data['y_pred'], \"weights\": df_data['scaleweights']})\n","dataframe_to_save2 = pd.DataFrame({'Type': 'Test Bkg', 'NN output': df_test['y_pred'][y_test==0], \"weights\": (df_test['scaleweights'][y_test==0])/3})\n","dataframe_to_save3 = pd.DataFrame({'Type': 'Test Sig', 'NN output': df_test['y_pred'][y_test==1], \"weights\": (df_test['scaleweights'][y_test==1])/3})\n","dataframe_to_save4 = pd.DataFrame({'Type': 'Train Bkg', 'NN output': df_train['y_pred'][y_train==0], \"weights\": (df_test['scaleweights'][y_train==0])})\n","dataframe_to_save5 = pd.DataFrame({'Type': 'Train Sig', 'NN output': df_train['y_pred'][y_train==1], \"weights\": (df_test['scaleweights'][y_train==1])})\n","dataframe_to_save6 = pd.DataFrame({'Type': 'Val Bkg', 'NN output': df_val['y_pred'][y_val==0], \"weights\": (df_test['scaleweights'][y_val==0])})\n","dataframe_to_save7 = pd.DataFrame({'Type': 'Val Sig', 'NN output': df_val['y_pred'][y_val==1], \"weights\": (df_test['scaleweights'][y_val==1])})\n","\n","dataframe_to_save = pd.concat([dataframe_to_save1, dataframe_to_save2, dataframe_to_save3, dataframe_to_save4, dataframe_to_save5, dataframe_to_save6, dataframe_to_save7])\n","dataframe_to_save.to_csv(OUT_DIR + \"NN_output.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataframe_to_save"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":696,"status":"ok","timestamp":1717847816805,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"sXcUtEa7Wd3o","outputId":"99cb5863-9849-43f3-b0d4-02b0d0f192a1"},"outputs":[],"source":["# Plot the distribution of the measured data, the signal and background\n","plt.hist(bins[:-1], bins, weights=N, histtype='step', color='blue', label='Measured Data')\n","plt.hist(bins[:-1], bins, weights=B, histtype='step', color='green', label='Background')\n","plt.hist(bins[:-1], bins, weights=S, histtype='step', color='red', label='Signal')\n","\n","plt.xlabel('NN output')\n","plt.ylabel('Events')\n","plt.title('NN output for Measured Data, Signal and Background (test set)')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_NSB.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the model to the data to obtain the background normalization factor epsilon, using the bins with less signal\n","\n","model_spec_discovery = {'channels': [{'name': 'singlechannel',\n","              'samples': [\n","              {'name': 'signal','data': S2.tolist(),\n","               'modifiers': []},\n","              {'name': 'bkg1','data': B2.tolist(),\n","               'modifiers': [{'data': None, 'name': 'epsilon', 'type': 'normfactor'}]},\n","              ]\n","              }],\n","              \"observations\": [{ \"name\": \"singlechannel\", \"data\": N2.tolist() }],\n","              \"measurements\": [{ \"name\": \"Measurement\", \"config\": {\"poi\": 'epsilon', \"parameters\": []}}],\n","\n","              \"version\": \"1.0.0\",\n","}\n","\n","workspace_discovery = pyhf.Workspace(model_spec_discovery)\n","model_discovery = workspace_discovery.model()\n","\n","print(\"## Model\")\n","print(f\"  channels: {model_discovery.config.channels}\")\n","print(f\"     nbins: {model_discovery.config.channel_nbins}\")\n","print(f\"   samples: {model_discovery.config.samples}\")\n","print(f\" modifiers: {model_discovery.config.modifiers}\")\n","print(f\"parameters: {model_discovery.config.parameters}\")\n","print(f\"par. order: {model_discovery.config.par_order}\")\n","\n","print(\"\\n## Model parameters\")\n","print(f'   default: {model_discovery.config.suggested_init()}')\n","print(f'    bounds: {model_discovery.config.suggested_bounds()}')\n","\n","data = N2.tolist() + model_discovery.config.auxdata\n","\n","test_stat = \"q0\"\n","test_poi = 0.\n","\n","best_pars = pyhf.infer.mle.fit(data=data, pdf=model_discovery)\n","print(\"\\nBest fit parameters:\")\n","print(f\"  epsilon: {best_pars[0]:.2e}\")\n","print(best_pars)\n","epsilon = best_pars[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the distribution of the measured data, the signal and normalized background\n","B3 = epsilon * B\n","\n","plt.hist(bins[:-1], bins, weights=N, histtype='step', color='blue', label='measured data')\n","plt.hist(bins[:-1], bins, weights=B3, histtype='step', color='green', label=r'bkg * $\\epsilon$ = bkg * %.2e' % epsilon)\n","plt.hist(bins[:-1], bins, weights=S, histtype='step', color='red', label='sig')\n","plt.xlabel('NN output')\n","plt.ylabel('Events')\n","plt.title('NN output for Measured Data, Signal and Normalized Background (test set)')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_NSB_discovery.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Exclusion Fit to obtain the upper limit on the signal strength\n","model_spec = {'channels': [{'name': 'singlechannel',\n","              'samples': [\n","              {'name': 'signal','data': S.tolist(),\n","               'modifiers': [{'data': None, 'name': 'mu', 'type': 'normfactor'}]},\n","              {'name': 'bkg1','data': B.tolist(),\n","               'modifiers': []},\n","              ]\n","              }],\n","              \"observations\": [{ \"name\": \"singlechannel\", \"data\": N.tolist() }],\n","              \"measurements\": [{ \"name\": \"Measurement\", \"config\": {\"poi\": 'mu', \"parameters\": []}}],\n","\n","              \"version\": \"1.0.0\",\n","}\n","\n","workspace = pyhf.Workspace(model_spec)\n","model = workspace.model()\n","\n","print(\"## Model\")\n","print(f\"  channels: {model.config.channels}\")\n","print(f\"     nbins: {model.config.channel_nbins}\")\n","print(f\"   samples: {model.config.samples}\")\n","print(f\" modifiers: {model.config.modifiers}\")\n","print(f\"parameters: {model.config.parameters}\")\n","print(f\"par. order: {model.config.par_order}\")\n","\n","print(\"\\n## Model parameters\")\n","print(f'   default: {model.config.suggested_init()}')\n","print(f'    bounds: {model.config.suggested_bounds()}')\n","\n","data = N.tolist() + model.config.auxdata\n","\n","test_stat = \"qtilde\"\n","test_poi = 1.\n","\n","# Vary mu values\n","poi_values = np.linspace(0.01, 5, 500)\n","obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(data, model, poi_values, level=0.05, return_results=True)\n","print(f\"\\nObserved μ upper limit (obs): {obs_limit:.3f}, Expected μ upper limit {exp_limits[2]:.3f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3164,"status":"ok","timestamp":1717847819890,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"k8NSvWFMWd3o","outputId":"fd6a5259-f935-4668-ba58-329dab18397b"},"outputs":[],"source":["# Exclusion Fit to obtain the upper limit on the signal strength\n","model_spec_norm = {'channels': [{'name': 'singlechannel',\n","              'samples': [\n","              {'name': 'signal','data': S.tolist(),\n","               'modifiers': [{'data': None, 'name': 'mu', 'type': 'normfactor'}]},\n","              {'name': 'bkg1','data': B3.tolist(),\n","               'modifiers': []},\n","              ]\n","              }],\n","              \"observations\": [{ \"name\": \"singlechannel\", \"data\": N.tolist() }],\n","              \"measurements\": [{ \"name\": \"Measurement\", \"config\": {\"poi\": 'mu', \"parameters\": []}}],\n","\n","              \"version\": \"1.0.0\",\n","}\n","\n","workspace = pyhf.Workspace(model_spec_norm)\n","model = workspace.model()\n","\n","print(\"## Model\")\n","print(f\"  channels: {model.config.channels}\")\n","print(f\"     nbins: {model.config.channel_nbins}\")\n","print(f\"   samples: {model.config.samples}\")\n","print(f\" modifiers: {model.config.modifiers}\")\n","print(f\"parameters: {model.config.parameters}\")\n","print(f\"par. order: {model.config.par_order}\")\n","\n","print(\"\\n## Model parameters\")\n","print(f'   default: {model.config.suggested_init()}')\n","print(f'    bounds: {model.config.suggested_bounds()}')\n","\n","data = N.tolist() + model.config.auxdata\n","\n","test_stat = \"qtilde\"\n","test_poi = 1.\n","\n","# Vary mu values\n","poi_values = np.linspace(0.01, 5, 500)\n","obs_limit_norm, exp_limits_norm, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(data, model, poi_values, level=0.05, return_results=True)\n","print(f\"\\nObserved μ upper limit (obs): {obs_limit_norm:.3f}, Expected μ upper limit {exp_limits_norm[2]:.3f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save result to txt\n","with open(OUT_DIR + \"results.txt\", \"w\") as f:\n","    f.write(f\"Value for epsilon (background normalization factor): {epsilon}\\n\")\n","    f.write(f\"Observed μ upper limit (obs) with epsilon: {obs_limit_norm}\\n\")\n","    f.write(f\"Expected μ upper limit (exp) with epsilon: {exp_limits_norm[2]}\\n\\n\\n\")\n","    f.write(f\"Observed μ upper limit (obs): {obs_limit}\\n\")\n","    f.write(f\"Expected μ upper limit (exp): {exp_limits[2]}\\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
