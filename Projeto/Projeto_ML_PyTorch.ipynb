{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataframe.csv')\n",
    "data.columns\n",
    "training_features = ['etmiss', 'mtw', 'leadleptPt', 'leadleptEta',\n",
    "       'leadleptE', 'leadleptPhi', 'n_TopLRjets',\n",
    "       'leadTopLRjet_pt', 'leadTopLRjet_eta', 'leadTopLRjet_phi',\n",
    "       'leadTopLRjet_m', 'leadTopLRjet_Tau32', 'n_jets', 'leadjet_pt',\n",
    "       'leadjet_eta', 'n_bjets', 'leadbjet_pt', 'leadbjet_eta', 'ttbarMLR']\n",
    "label = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data[training_features] = scaler.fit_transform(data[training_features])\n",
    "\n",
    "X = scaler.transform(data[training_features])\n",
    "y = data[label]\n",
    "w = data['scaleweight']\n",
    "\n",
    "x_train, x_test, y_train, y_test, w_train, w_test = train_test_split(X, y, w, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201676,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to(torch.device(\"mps\"))\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32).to(torch.device(\"mps\"))\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(torch.device(\"mps\"))\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).view(-1, 1).to(torch.device(\"mps\"))\n",
    "w_train_tensor = torch.tensor(w_train, dtype=torch.float32).to(torch.device(\"mps\"))\n",
    "w_test_tensor = torch.tensor(w_test.to_numpy(), dtype=torch.float32).to(torch.device(\"mps\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50420\n",
      "Net(\n",
      "  (fc1): Linear(in_features=19, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Training data shape: torch.Size([201676, 19])\n",
      "Testing data shape: torch.Size([50420, 19])\n",
      "Number of parameters: 5505\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "print(x_test_tensor.shape[0])\n",
    "net = Net(x_test_tensor.shape[1]).to(torch.device(\"mps\"))\n",
    "# Define the loss function and the optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "print(net)\n",
    "print(f\"Training data shape: {x_train_tensor.shape}\")\n",
    "print(f\"Testing data shape: {x_test_tensor.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in net.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, X_train, y_train, X_test, y_test, epochs=50):\n",
    "    \"\"\"\n",
    "        Defining the training loop. \n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    loss_history_test = []\n",
    "    for _ in range(epochs):\n",
    "        model.train()                                       # sets the model to training mode.\n",
    "        optimizer.zero_grad()                               # sets the gradients \"to zero\".\n",
    "\n",
    "        y_ = model(X_train)                                 # forward pass (y predicted).\n",
    "        loss = loss_fn(y_, y_train)                         # computes the loss.\n",
    "        \n",
    "        loss_history.append(loss.item())                    # appends the loss to the loss history.\n",
    "\n",
    "        loss.backward()                                     # computes the gradients.\n",
    "        optimizer.step()                                    # updates weights using the gradients.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test)                         # forward pass (y predicted).\n",
    "            test_loss = loss_fn(outputs, y_test)            # computes the loss.\n",
    "            loss_history_test.append(test_loss.item())      # appends the loss to the loss history.\n",
    "\n",
    "        print(\"Epoch: {}, Training Loss: {}, Testing Loss: {}\".format(_, loss.item(), test_loss.item()))\n",
    "    return loss_history, loss_history_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X):\n",
    "    \"\"\"\n",
    "        Evaluating the model. \n",
    "    \"\"\"\n",
    "    model.eval()  # <-- here\n",
    "    with torch.no_grad(): \n",
    "        y_ = model(X)    \n",
    "    return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.31910455226898193, Testing Loss: 0.2905193865299225\n",
      "Epoch: 1, Training Loss: 0.29028820991516113, Testing Loss: 0.26302242279052734\n",
      "Epoch: 2, Training Loss: 0.2628863453865051, Testing Loss: 0.2370922863483429\n",
      "Epoch: 3, Training Loss: 0.23705428838729858, Testing Loss: 0.2127978503704071\n",
      "Epoch: 4, Training Loss: 0.21285879611968994, Testing Loss: 0.19015085697174072\n",
      "Epoch: 5, Training Loss: 0.19030895829200745, Testing Loss: 0.16913078725337982\n",
      "Epoch: 6, Training Loss: 0.1693868339061737, Testing Loss: 0.14975126087665558\n",
      "Epoch: 7, Training Loss: 0.15010294318199158, Testing Loss: 0.1320471614599228\n",
      "Epoch: 8, Training Loss: 0.13249176740646362, Testing Loss: 0.11604893952608109\n",
      "Epoch: 9, Training Loss: 0.11658287793397903, Testing Loss: 0.10175929963588715\n",
      "Epoch: 10, Training Loss: 0.10237793624401093, Testing Loss: 0.08915609866380692\n",
      "Epoch: 11, Training Loss: 0.08985291421413422, Testing Loss: 0.07818984985351562\n",
      "Epoch: 12, Training Loss: 0.07896088808774948, Testing Loss: 0.06879109144210815\n",
      "Epoch: 13, Training Loss: 0.06963171809911728, Testing Loss: 0.06087304279208183\n",
      "Epoch: 14, Training Loss: 0.06177857518196106, Testing Loss: 0.054323710501194\n",
      "Epoch: 15, Training Loss: 0.05528935790061951, Testing Loss: 0.04900645464658737\n",
      "Epoch: 16, Training Loss: 0.05002618953585625, Testing Loss: 0.04476376995444298\n",
      "Epoch: 17, Training Loss: 0.04583263024687767, Testing Loss: 0.041433632373809814\n",
      "Epoch: 18, Training Loss: 0.04254686087369919, Testing Loss: 0.038859352469444275\n",
      "Epoch: 19, Training Loss: 0.04001307487487793, Testing Loss: 0.0368988998234272\n",
      "Epoch: 20, Training Loss: 0.03808858245611191, Testing Loss: 0.035425685346126556\n",
      "Epoch: 21, Training Loss: 0.03664702922105789, Testing Loss: 0.03433136269450188\n",
      "Epoch: 22, Training Loss: 0.0355803556740284, Testing Loss: 0.03352658823132515\n",
      "Epoch: 23, Training Loss: 0.034799572080373764, Testing Loss: 0.032939884811639786\n",
      "Epoch: 24, Training Loss: 0.03423348441720009, Testing Loss: 0.03251514956355095\n",
      "Epoch: 25, Training Loss: 0.033826567232608795, Testing Loss: 0.03220972418785095\n",
      "Epoch: 26, Training Loss: 0.03353654593229294, Testing Loss: 0.03199148550629616\n",
      "Epoch: 27, Training Loss: 0.03333166241645813, Testing Loss: 0.03183669224381447\n",
      "Epoch: 28, Training Loss: 0.033188361674547195, Testing Loss: 0.031727880239486694\n",
      "Epoch: 29, Training Loss: 0.03308941423892975, Testing Loss: 0.03165225312113762\n",
      "Epoch: 30, Training Loss: 0.03302225470542908, Testing Loss: 0.03160044178366661\n",
      "Epoch: 31, Training Loss: 0.032977744936943054, Testing Loss: 0.031565699726343155\n",
      "Epoch: 32, Training Loss: 0.032949306070804596, Testing Loss: 0.03154315799474716\n",
      "Epoch: 33, Training Loss: 0.032932210713624954, Testing Loss: 0.031529270112514496\n",
      "Epoch: 34, Training Loss: 0.03292303904891014, Testing Loss: 0.03152148425579071\n",
      "Epoch: 35, Training Loss: 0.03291933983564377, Testing Loss: 0.03151794150471687\n",
      "Epoch: 36, Training Loss: 0.032919347286224365, Testing Loss: 0.03151731193065643\n",
      "Epoch: 37, Training Loss: 0.03292180597782135, Testing Loss: 0.03151863068342209\n",
      "Epoch: 38, Training Loss: 0.032925814390182495, Testing Loss: 0.03152121230959892\n",
      "Epoch: 39, Training Loss: 0.03293074294924736, Testing Loss: 0.03152455389499664\n",
      "Epoch: 40, Training Loss: 0.032936133444309235, Testing Loss: 0.031528301537036896\n",
      "Epoch: 41, Training Loss: 0.03294166550040245, Testing Loss: 0.031532205641269684\n",
      "Epoch: 42, Training Loss: 0.032947130501270294, Testing Loss: 0.03153609111905098\n",
      "Epoch: 43, Training Loss: 0.03295237198472023, Testing Loss: 0.031539835035800934\n",
      "Epoch: 44, Training Loss: 0.032957300543785095, Testing Loss: 0.03154335170984268\n",
      "Epoch: 45, Training Loss: 0.032961856573820114, Testing Loss: 0.031546592712402344\n",
      "Epoch: 46, Training Loss: 0.0329660028219223, Testing Loss: 0.03154952451586723\n",
      "Epoch: 47, Training Loss: 0.032969724386930466, Testing Loss: 0.031552135944366455\n",
      "Epoch: 48, Training Loss: 0.032973021268844604, Testing Loss: 0.03155441954731941\n",
      "Epoch: 49, Training Loss: 0.03297589346766472, Testing Loss: 0.0315563790500164\n",
      "Epoch: 50, Training Loss: 0.032978355884552, Testing Loss: 0.03155803307890892\n",
      "Epoch: 51, Training Loss: 0.032980430871248245, Testing Loss: 0.031559377908706665\n",
      "Epoch: 52, Training Loss: 0.03298213705420494, Testing Loss: 0.03156043216586113\n",
      "Epoch: 53, Training Loss: 0.032983485609292984, Testing Loss: 0.03156121075153351\n",
      "Epoch: 54, Training Loss: 0.03298450633883476, Testing Loss: 0.031561728566884995\n",
      "Epoch: 55, Training Loss: 0.03298522159457207, Testing Loss: 0.03156200796365738\n",
      "Epoch: 56, Training Loss: 0.03298564255237579, Testing Loss: 0.03156205639243126\n",
      "Epoch: 57, Training Loss: 0.03298579528927803, Testing Loss: 0.03156188130378723\n",
      "Epoch: 58, Training Loss: 0.03298569098114967, Testing Loss: 0.03156150132417679\n",
      "Epoch: 59, Training Loss: 0.03298535197973251, Testing Loss: 0.03156092390418053\n",
      "Epoch: 60, Training Loss: 0.03298478573560715, Testing Loss: 0.031560152769088745\n",
      "Epoch: 61, Training Loss: 0.032984018325805664, Testing Loss: 0.03155921772122383\n",
      "Epoch: 62, Training Loss: 0.03298305347561836, Testing Loss: 0.031558115035295486\n",
      "Epoch: 63, Training Loss: 0.032981906086206436, Testing Loss: 0.0315568782389164\n",
      "Epoch: 64, Training Loss: 0.03298059105873108, Testing Loss: 0.031555499881505966\n",
      "Epoch: 65, Training Loss: 0.03297911956906319, Testing Loss: 0.03155399486422539\n",
      "Epoch: 66, Training Loss: 0.03297751024365425, Testing Loss: 0.03155237063765526\n",
      "Epoch: 67, Training Loss: 0.03297576680779457, Testing Loss: 0.03155062347650528\n",
      "Epoch: 68, Training Loss: 0.03297390788793564, Testing Loss: 0.03154876455664635\n",
      "Epoch: 69, Training Loss: 0.03297193720936775, Testing Loss: 0.03154683858156204\n",
      "Epoch: 70, Training Loss: 0.03296985849738121, Testing Loss: 0.03154480457305908\n",
      "Epoch: 71, Training Loss: 0.032967690378427505, Testing Loss: 0.03154270350933075\n",
      "Epoch: 72, Training Loss: 0.03296542912721634, Testing Loss: 0.03154052048921585\n",
      "Epoch: 73, Training Loss: 0.03296307474374771, Testing Loss: 0.031538259238004684\n",
      "Epoch: 74, Training Loss: 0.03296063467860222, Testing Loss: 0.03153591603040695\n",
      "Epoch: 75, Training Loss: 0.03295811638236046, Testing Loss: 0.03153349459171295\n",
      "Epoch: 76, Training Loss: 0.03295552730560303, Testing Loss: 0.031531013548374176\n",
      "Epoch: 77, Training Loss: 0.03295287489891052, Testing Loss: 0.03152848035097122\n",
      "Epoch: 78, Training Loss: 0.03295014426112175, Testing Loss: 0.03152590990066528\n",
      "Epoch: 79, Training Loss: 0.032947346568107605, Testing Loss: 0.03152327612042427\n",
      "Epoch: 80, Training Loss: 0.03294449299573898, Testing Loss: 0.03152058273553848\n",
      "Epoch: 81, Training Loss: 0.03294157609343529, Testing Loss: 0.03151783347129822\n",
      "Epoch: 82, Training Loss: 0.032938603311777115, Testing Loss: 0.031515058130025864\n",
      "Epoch: 83, Training Loss: 0.03293559327721596, Testing Loss: 0.03151226043701172\n",
      "Epoch: 84, Training Loss: 0.032932527363300323, Testing Loss: 0.03150944039225578\n",
      "Epoch: 85, Training Loss: 0.03292941302061081, Testing Loss: 0.031506601721048355\n",
      "Epoch: 86, Training Loss: 0.03292625769972801, Testing Loss: 0.03150373697280884\n",
      "Epoch: 87, Training Loss: 0.03292304277420044, Testing Loss: 0.03150083124637604\n",
      "Epoch: 88, Training Loss: 0.032919786870479584, Testing Loss: 0.03149787336587906\n",
      "Epoch: 89, Training Loss: 0.03291648253798485, Testing Loss: 0.0314948596060276\n",
      "Epoch: 90, Training Loss: 0.032913144677877426, Testing Loss: 0.03149180859327316\n",
      "Epoch: 91, Training Loss: 0.03290976956486702, Testing Loss: 0.03148871287703514\n",
      "Epoch: 92, Training Loss: 0.03290636092424393, Testing Loss: 0.03148562088608742\n",
      "Epoch: 93, Training Loss: 0.03290291503071785, Testing Loss: 0.03148248791694641\n",
      "Epoch: 94, Training Loss: 0.03289944306015968, Testing Loss: 0.0314793586730957\n",
      "Epoch: 95, Training Loss: 0.03289593383669853, Testing Loss: 0.0314762219786644\n",
      "Epoch: 96, Training Loss: 0.03289240226149559, Testing Loss: 0.0314730703830719\n",
      "Epoch: 97, Training Loss: 0.03288884088397026, Testing Loss: 0.03146990016102791\n",
      "Epoch: 98, Training Loss: 0.032885242253541946, Testing Loss: 0.03146670013666153\n",
      "Epoch: 99, Training Loss: 0.032881610095500946, Testing Loss: 0.03146347776055336\n",
      "Epoch: 100, Training Loss: 0.032877951860427856, Testing Loss: 0.031460240483284\n",
      "Epoch: 101, Training Loss: 0.03287426382303238, Testing Loss: 0.031456977128982544\n",
      "Epoch: 102, Training Loss: 0.03287053853273392, Testing Loss: 0.0314536951482296\n",
      "Epoch: 103, Training Loss: 0.03286678344011307, Testing Loss: 0.03145042806863785\n",
      "Epoch: 104, Training Loss: 0.03286299854516983, Testing Loss: 0.0314471572637558\n",
      "Epoch: 105, Training Loss: 0.03285918012261391, Testing Loss: 0.031443867832422256\n",
      "Epoch: 106, Training Loss: 0.032855335623025894, Testing Loss: 0.03144055977463722\n",
      "Epoch: 107, Training Loss: 0.032851461321115494, Testing Loss: 0.0314372181892395\n",
      "Epoch: 108, Training Loss: 0.032847560942173004, Testing Loss: 0.031433816999197006\n",
      "Epoch: 109, Training Loss: 0.03284363076090813, Testing Loss: 0.03143034130334854\n",
      "Epoch: 110, Training Loss: 0.03283967077732086, Testing Loss: 0.031426794826984406\n",
      "Epoch: 111, Training Loss: 0.032835666090250015, Testing Loss: 0.031423166394233704\n",
      "Epoch: 112, Training Loss: 0.03283162787556648, Testing Loss: 0.03141948580741882\n",
      "Epoch: 113, Training Loss: 0.03282755985856056, Testing Loss: 0.03141576051712036\n",
      "Epoch: 114, Training Loss: 0.03282347321510315, Testing Loss: 0.03141201660037041\n",
      "Epoch: 115, Training Loss: 0.03281935304403305, Testing Loss: 0.03140827640891075\n",
      "Epoch: 116, Training Loss: 0.03281521052122116, Testing Loss: 0.03140454366803169\n",
      "Epoch: 117, Training Loss: 0.032811038196086884, Testing Loss: 0.031400829553604126\n",
      "Epoch: 118, Training Loss: 0.03280683606863022, Testing Loss: 0.03139711543917656\n",
      "Epoch: 119, Training Loss: 0.032802607864141464, Testing Loss: 0.0313933864235878\n",
      "Epoch: 120, Training Loss: 0.03279834985733032, Testing Loss: 0.031389620155096054\n",
      "Epoch: 121, Training Loss: 0.03279407322406769, Testing Loss: 0.03138580173254013\n",
      "Epoch: 122, Training Loss: 0.03278976306319237, Testing Loss: 0.03138192370533943\n",
      "Epoch: 123, Training Loss: 0.032785430550575256, Testing Loss: 0.03137804567813873\n",
      "Epoch: 124, Training Loss: 0.03278107941150665, Testing Loss: 0.03137415647506714\n",
      "Epoch: 125, Training Loss: 0.03277670964598656, Testing Loss: 0.03137027099728584\n",
      "Epoch: 126, Training Loss: 0.03277232125401497, Testing Loss: 0.03136640042066574\n",
      "Epoch: 127, Training Loss: 0.032767925411462784, Testing Loss: 0.031362541019916534\n",
      "Epoch: 128, Training Loss: 0.0327635183930397, Testing Loss: 0.03135870397090912\n",
      "Epoch: 129, Training Loss: 0.03275911137461662, Testing Loss: 0.031354870647192\n",
      "Epoch: 130, Training Loss: 0.032754700630903244, Testing Loss: 0.0313509926199913\n",
      "Epoch: 131, Training Loss: 0.03275028243660927, Testing Loss: 0.03134707733988762\n",
      "Epoch: 132, Training Loss: 0.0327458530664444, Testing Loss: 0.03134314715862274\n",
      "Epoch: 133, Training Loss: 0.032741427421569824, Testing Loss: 0.03133922442793846\n",
      "Epoch: 134, Training Loss: 0.03273700922727585, Testing Loss: 0.03133535385131836\n",
      "Epoch: 135, Training Loss: 0.03273260220885277, Testing Loss: 0.031331516802310944\n",
      "Epoch: 136, Training Loss: 0.03272818773984909, Testing Loss: 0.031327687203884125\n",
      "Epoch: 137, Training Loss: 0.03272378444671631, Testing Loss: 0.031323861330747604\n",
      "Epoch: 138, Training Loss: 0.03271938115358353, Testing Loss: 0.031320031732320786\n",
      "Epoch: 139, Training Loss: 0.03271498531103134, Testing Loss: 0.031316183507442474\n",
      "Epoch: 140, Training Loss: 0.032710593193769455, Testing Loss: 0.03131233900785446\n",
      "Epoch: 141, Training Loss: 0.03270621597766876, Testing Loss: 0.03130851686000824\n",
      "Epoch: 142, Training Loss: 0.03270186111330986, Testing Loss: 0.031304702162742615\n",
      "Epoch: 143, Training Loss: 0.032697513699531555, Testing Loss: 0.03130090609192848\n",
      "Epoch: 144, Training Loss: 0.03269318863749504, Testing Loss: 0.03129709139466286\n",
      "Epoch: 145, Training Loss: 0.03268887847661972, Testing Loss: 0.03129331022500992\n",
      "Epoch: 146, Training Loss: 0.03268460929393768, Testing Loss: 0.03128964081406593\n",
      "Epoch: 147, Training Loss: 0.03268035128712654, Testing Loss: 0.0312860906124115\n",
      "Epoch: 148, Training Loss: 0.032676126807928085, Testing Loss: 0.031282637268304825\n",
      "Epoch: 149, Training Loss: 0.032671935856342316, Testing Loss: 0.03127920255064964\n",
      "Epoch: 150, Training Loss: 0.03266778960824013, Testing Loss: 0.031275711953639984\n",
      "Epoch: 151, Training Loss: 0.03266366571187973, Testing Loss: 0.03127212077379227\n",
      "Epoch: 152, Training Loss: 0.03265957161784172, Testing Loss: 0.03126848489046097\n",
      "Epoch: 153, Training Loss: 0.0326555110514164, Testing Loss: 0.031264837831258774\n",
      "Epoch: 154, Training Loss: 0.03265148773789406, Testing Loss: 0.031261276453733444\n",
      "Epoch: 155, Training Loss: 0.03264748677611351, Testing Loss: 0.031257808208465576\n",
      "Epoch: 156, Training Loss: 0.03264351189136505, Testing Loss: 0.03125442937016487\n",
      "Epoch: 157, Training Loss: 0.03263956308364868, Testing Loss: 0.03125116974115372\n",
      "Epoch: 158, Training Loss: 0.032635632902383804, Testing Loss: 0.031247971579432487\n",
      "Epoch: 159, Training Loss: 0.032631728798151016, Testing Loss: 0.03124469332396984\n",
      "Epoch: 160, Training Loss: 0.03262784704566002, Testing Loss: 0.03124130889773369\n",
      "Epoch: 161, Training Loss: 0.032623983919620514, Testing Loss: 0.031237924471497536\n",
      "Epoch: 162, Training Loss: 0.0326201431453228, Testing Loss: 0.031234582886099815\n",
      "Epoch: 163, Training Loss: 0.032616324722766876, Testing Loss: 0.03123125247657299\n",
      "Epoch: 164, Training Loss: 0.03261251375079155, Testing Loss: 0.031227989122271538\n",
      "Epoch: 165, Training Loss: 0.032608725130558014, Testing Loss: 0.031224871054291725\n",
      "Epoch: 166, Training Loss: 0.032604947686195374, Testing Loss: 0.031221726909279823\n",
      "Epoch: 167, Training Loss: 0.032601192593574524, Testing Loss: 0.031218474730849266\n",
      "Epoch: 168, Training Loss: 0.032597437500953674, Testing Loss: 0.031215112656354904\n",
      "Epoch: 169, Training Loss: 0.03259369730949402, Testing Loss: 0.031211713328957558\n",
      "Epoch: 170, Training Loss: 0.03258996829390526, Testing Loss: 0.031208448112010956\n",
      "Epoch: 171, Training Loss: 0.03258625045418739, Testing Loss: 0.03120538964867592\n",
      "Epoch: 172, Training Loss: 0.03258253633975983, Testing Loss: 0.031202342361211777\n",
      "Epoch: 173, Training Loss: 0.032578837126493454, Testing Loss: 0.03119916282594204\n",
      "Epoch: 174, Training Loss: 0.03257513791322708, Testing Loss: 0.03119583986699581\n",
      "Epoch: 175, Training Loss: 0.03257144242525101, Testing Loss: 0.031192516908049583\n",
      "Epoch: 176, Training Loss: 0.032567765563726425, Testing Loss: 0.03118935227394104\n",
      "Epoch: 177, Training Loss: 0.032564084976911545, Testing Loss: 0.03118620067834854\n",
      "Epoch: 178, Training Loss: 0.03256041184067726, Testing Loss: 0.031183090060949326\n",
      "Epoch: 179, Training Loss: 0.03255673870444298, Testing Loss: 0.03118003159761429\n",
      "Epoch: 180, Training Loss: 0.03255308046936989, Testing Loss: 0.031176915392279625\n",
      "Epoch: 181, Training Loss: 0.0325494185090065, Testing Loss: 0.031173789873719215\n",
      "Epoch: 182, Training Loss: 0.03254576399922371, Testing Loss: 0.031170669943094254\n",
      "Epoch: 183, Training Loss: 0.032542113214731216, Testing Loss: 0.0311675276607275\n",
      "Epoch: 184, Training Loss: 0.032538462430238724, Testing Loss: 0.03116433136165142\n",
      "Epoch: 185, Training Loss: 0.03253481537103653, Testing Loss: 0.031161164864897728\n",
      "Epoch: 186, Training Loss: 0.03253116458654404, Testing Loss: 0.03115793690085411\n",
      "Epoch: 187, Training Loss: 0.032527513802051544, Testing Loss: 0.031154591590166092\n",
      "Epoch: 188, Training Loss: 0.032523855566978455, Testing Loss: 0.031151197850704193\n",
      "Epoch: 189, Training Loss: 0.03252020105719566, Testing Loss: 0.031147964298725128\n",
      "Epoch: 190, Training Loss: 0.032516539096832275, Testing Loss: 0.031144963577389717\n",
      "Epoch: 191, Training Loss: 0.03251287341117859, Testing Loss: 0.031142044812440872\n",
      "Epoch: 192, Training Loss: 0.0325092077255249, Testing Loss: 0.031139060854911804\n",
      "Epoch: 193, Training Loss: 0.03250555694103241, Testing Loss: 0.03113585151731968\n",
      "Epoch: 194, Training Loss: 0.03250189125537872, Testing Loss: 0.03113252855837345\n",
      "Epoch: 195, Training Loss: 0.03249822556972504, Testing Loss: 0.03112928383052349\n",
      "Epoch: 196, Training Loss: 0.03249456733465195, Testing Loss: 0.03112630546092987\n",
      "Epoch: 197, Training Loss: 0.03249089792370796, Testing Loss: 0.031123541295528412\n",
      "Epoch: 198, Training Loss: 0.032487235963344574, Testing Loss: 0.031120536848902702\n",
      "Epoch: 199, Training Loss: 0.032483574002981186, Testing Loss: 0.031117070466279984\n",
      "Epoch: 200, Training Loss: 0.03247988969087601, Testing Loss: 0.031113557517528534\n",
      "Epoch: 201, Training Loss: 0.032476212829351425, Testing Loss: 0.031110184267163277\n",
      "Epoch: 202, Training Loss: 0.03247253969311714, Testing Loss: 0.03110702708363533\n",
      "Epoch: 203, Training Loss: 0.032468847930431366, Testing Loss: 0.031104037538170815\n",
      "Epoch: 204, Training Loss: 0.03246515244245529, Testing Loss: 0.031100964173674583\n",
      "Epoch: 205, Training Loss: 0.03246146813035011, Testing Loss: 0.031097466126084328\n",
      "Epoch: 206, Training Loss: 0.03245776519179344, Testing Loss: 0.0310939010232687\n",
      "Epoch: 207, Training Loss: 0.03245406225323677, Testing Loss: 0.0310903899371624\n",
      "Epoch: 208, Training Loss: 0.0324503555893898, Testing Loss: 0.0310872420668602\n",
      "Epoch: 209, Training Loss: 0.032446637749671936, Testing Loss: 0.031084172427654266\n",
      "Epoch: 210, Training Loss: 0.03244292736053467, Testing Loss: 0.031080933287739754\n",
      "Epoch: 211, Training Loss: 0.032439205795526505, Testing Loss: 0.031077701598405838\n",
      "Epoch: 212, Training Loss: 0.032435495406389236, Testing Loss: 0.031074298545718193\n",
      "Epoch: 213, Training Loss: 0.032431770116090775, Testing Loss: 0.031070861965417862\n",
      "Epoch: 214, Training Loss: 0.03242805227637291, Testing Loss: 0.031067466363310814\n",
      "Epoch: 215, Training Loss: 0.032424334436655045, Testing Loss: 0.031064555048942566\n",
      "Epoch: 216, Training Loss: 0.03242060914635658, Testing Loss: 0.031061528250575066\n",
      "Epoch: 217, Training Loss: 0.03241690993309021, Testing Loss: 0.031058331951498985\n",
      "Epoch: 218, Training Loss: 0.03241320326924324, Testing Loss: 0.0310550294816494\n",
      "Epoch: 219, Training Loss: 0.03240950033068657, Testing Loss: 0.03105173073709011\n",
      "Epoch: 220, Training Loss: 0.0324057936668396, Testing Loss: 0.031048351898789406\n",
      "Epoch: 221, Training Loss: 0.03240209072828293, Testing Loss: 0.031044935807585716\n",
      "Epoch: 222, Training Loss: 0.03239841014146805, Testing Loss: 0.031041817739605904\n",
      "Epoch: 223, Training Loss: 0.032394737005233765, Testing Loss: 0.031038718298077583\n",
      "Epoch: 224, Training Loss: 0.032391078770160675, Testing Loss: 0.031035374850034714\n",
      "Epoch: 225, Training Loss: 0.03238743543624878, Testing Loss: 0.03103197179734707\n",
      "Epoch: 226, Training Loss: 0.03238379955291748, Testing Loss: 0.03102860413491726\n",
      "Epoch: 227, Training Loss: 0.03238016739487648, Testing Loss: 0.031025219708681107\n",
      "Epoch: 228, Training Loss: 0.032376550137996674, Testing Loss: 0.03102179802954197\n",
      "Epoch: 229, Training Loss: 0.032372940331697464, Testing Loss: 0.031018495559692383\n",
      "Epoch: 230, Training Loss: 0.03236934170126915, Testing Loss: 0.031015215441584587\n",
      "Epoch: 231, Training Loss: 0.03236575424671173, Testing Loss: 0.031011639162898064\n",
      "Epoch: 232, Training Loss: 0.03236217796802521, Testing Loss: 0.031008388847112656\n",
      "Epoch: 233, Training Loss: 0.032358624041080475, Testing Loss: 0.031005317345261574\n",
      "Epoch: 234, Training Loss: 0.03235508129000664, Testing Loss: 0.03100213222205639\n",
      "Epoch: 235, Training Loss: 0.032351549714803696, Testing Loss: 0.030998820438981056\n",
      "Epoch: 236, Training Loss: 0.03234802931547165, Testing Loss: 0.030995436012744904\n",
      "Epoch: 237, Training Loss: 0.0323445200920105, Testing Loss: 0.030992215499281883\n",
      "Epoch: 238, Training Loss: 0.03234102204442024, Testing Loss: 0.030989088118076324\n",
      "Epoch: 239, Training Loss: 0.032337531447410583, Testing Loss: 0.03098599798977375\n",
      "Epoch: 240, Training Loss: 0.03233404457569122, Testing Loss: 0.03098289482295513\n",
      "Epoch: 241, Training Loss: 0.03233056887984276, Testing Loss: 0.030979899689555168\n",
      "Epoch: 242, Training Loss: 0.032327115535736084, Testing Loss: 0.030976662412285805\n",
      "Epoch: 243, Training Loss: 0.03232366591691971, Testing Loss: 0.030973244458436966\n",
      "Epoch: 244, Training Loss: 0.032320231199264526, Testing Loss: 0.03097008913755417\n",
      "Epoch: 245, Training Loss: 0.03231680765748024, Testing Loss: 0.03096737712621689\n",
      "Epoch: 246, Training Loss: 0.03231339529156685, Testing Loss: 0.030964717268943787\n",
      "Epoch: 247, Training Loss: 0.03231000527739525, Testing Loss: 0.03096175752580166\n",
      "Epoch: 248, Training Loss: 0.03230661898851395, Testing Loss: 0.030958406627178192\n",
      "Epoch: 249, Training Loss: 0.03230323642492294, Testing Loss: 0.03095472790300846\n",
      "Epoch: 250, Training Loss: 0.03229987621307373, Testing Loss: 0.030951660126447678\n",
      "Epoch: 251, Training Loss: 0.03229653462767601, Testing Loss: 0.03094911016523838\n",
      "Epoch: 252, Training Loss: 0.03229320049285889, Testing Loss: 0.030946752056479454\n",
      "Epoch: 253, Training Loss: 0.03228989988565445, Testing Loss: 0.0309437308460474\n",
      "Epoch: 254, Training Loss: 0.03228661045432091, Testing Loss: 0.03094022534787655\n",
      "Epoch: 255, Training Loss: 0.03228332847356796, Testing Loss: 0.030937010422348976\n",
      "Epoch: 256, Training Loss: 0.03228006511926651, Testing Loss: 0.030934341251850128\n",
      "Epoch: 257, Training Loss: 0.03227680176496506, Testing Loss: 0.030932020395994186\n",
      "Epoch: 258, Training Loss: 0.032273586839437485, Testing Loss: 0.030928878113627434\n",
      "Epoch: 259, Training Loss: 0.03227036073803902, Testing Loss: 0.0309253241866827\n",
      "Epoch: 260, Training Loss: 0.03226717561483383, Testing Loss: 0.030922502279281616\n",
      "Epoch: 261, Training Loss: 0.03226400166749954, Testing Loss: 0.03092019259929657\n",
      "Epoch: 262, Training Loss: 0.03226083517074585, Testing Loss: 0.030918063595891\n",
      "Epoch: 263, Training Loss: 0.032257720828056335, Testing Loss: 0.03091500699520111\n",
      "Epoch: 264, Training Loss: 0.032254595309495926, Testing Loss: 0.030911482870578766\n",
      "Epoch: 265, Training Loss: 0.03225148096680641, Testing Loss: 0.030908208340406418\n",
      "Epoch: 266, Training Loss: 0.03224840387701988, Testing Loss: 0.03090554103255272\n",
      "Epoch: 267, Training Loss: 0.03224532678723335, Testing Loss: 0.030903270468115807\n",
      "Epoch: 268, Training Loss: 0.03224227577447891, Testing Loss: 0.03090055286884308\n",
      "Epoch: 269, Training Loss: 0.03223925083875656, Testing Loss: 0.030897503718733788\n",
      "Epoch: 270, Training Loss: 0.032236248254776, Testing Loss: 0.030894681811332703\n",
      "Epoch: 271, Training Loss: 0.03223327174782753, Testing Loss: 0.0308920256793499\n",
      "Epoch: 272, Training Loss: 0.032230306416749954, Testing Loss: 0.03088963031768799\n",
      "Epoch: 273, Training Loss: 0.032227370887994766, Testing Loss: 0.03088722564280033\n",
      "Epoch: 274, Training Loss: 0.03222447261214256, Testing Loss: 0.03088447079062462\n",
      "Epoch: 275, Training Loss: 0.032221585512161255, Testing Loss: 0.03088158741593361\n",
      "Epoch: 276, Training Loss: 0.03221871703863144, Testing Loss: 0.030878696590662003\n",
      "Epoch: 277, Training Loss: 0.03221587464213371, Testing Loss: 0.030875911936163902\n",
      "Epoch: 278, Training Loss: 0.03221304714679718, Testing Loss: 0.030873367562890053\n",
      "Epoch: 279, Training Loss: 0.03221023827791214, Testing Loss: 0.03087095357477665\n",
      "Epoch: 280, Training Loss: 0.03220745548605919, Testing Loss: 0.03086840733885765\n",
      "Epoch: 281, Training Loss: 0.032204706221818924, Testing Loss: 0.030865952372550964\n",
      "Epoch: 282, Training Loss: 0.03220197558403015, Testing Loss: 0.030863534659147263\n",
      "Epoch: 283, Training Loss: 0.03219926729798317, Testing Loss: 0.03086111508309841\n",
      "Epoch: 284, Training Loss: 0.03219658508896828, Testing Loss: 0.030858824029564857\n",
      "Epoch: 285, Training Loss: 0.03219393268227577, Testing Loss: 0.030856400728225708\n",
      "Epoch: 286, Training Loss: 0.03219130262732506, Testing Loss: 0.03085407428443432\n",
      "Epoch: 287, Training Loss: 0.032188694924116135, Testing Loss: 0.03085179626941681\n",
      "Epoch: 288, Training Loss: 0.032186109572649, Testing Loss: 0.030849600210785866\n",
      "Epoch: 289, Training Loss: 0.03218355029821396, Testing Loss: 0.030847372487187386\n",
      "Epoch: 290, Training Loss: 0.03218100592494011, Testing Loss: 0.03084510937333107\n",
      "Epoch: 291, Training Loss: 0.03217848762869835, Testing Loss: 0.03084287792444229\n",
      "Epoch: 292, Training Loss: 0.032175980508327484, Testing Loss: 0.03084072843194008\n",
      "Epoch: 293, Training Loss: 0.03217350319027901, Testing Loss: 0.03083868883550167\n",
      "Epoch: 294, Training Loss: 0.03217105194926262, Testing Loss: 0.030836334452033043\n",
      "Epoch: 295, Training Loss: 0.03216861933469772, Testing Loss: 0.03083386830985546\n",
      "Epoch: 296, Training Loss: 0.032166216522455215, Testing Loss: 0.030831655487418175\n",
      "Epoch: 297, Training Loss: 0.0321638360619545, Testing Loss: 0.030829858034849167\n",
      "Epoch: 298, Training Loss: 0.032161466777324677, Testing Loss: 0.030827902257442474\n",
      "Epoch: 299, Training Loss: 0.03215913102030754, Testing Loss: 0.030825577676296234\n",
      "Epoch: 300, Training Loss: 0.0321568101644516, Testing Loss: 0.03082316555082798\n",
      "Epoch: 301, Training Loss: 0.032154519110918045, Testing Loss: 0.030821016058325768\n",
      "Epoch: 302, Training Loss: 0.03215225040912628, Testing Loss: 0.030819090083241463\n",
      "Epoch: 303, Training Loss: 0.03214999660849571, Testing Loss: 0.03081703558564186\n",
      "Epoch: 304, Training Loss: 0.03214778006076813, Testing Loss: 0.03081480972468853\n",
      "Epoch: 305, Training Loss: 0.032145582139492035, Testing Loss: 0.030812513083219528\n",
      "Epoch: 306, Training Loss: 0.03214341029524803, Testing Loss: 0.030810732394456863\n",
      "Epoch: 307, Training Loss: 0.03214125335216522, Testing Loss: 0.0308088231831789\n",
      "Epoch: 308, Training Loss: 0.0321391262114048, Testing Loss: 0.03080659918487072\n",
      "Epoch: 309, Training Loss: 0.03213702142238617, Testing Loss: 0.030804820358753204\n",
      "Epoch: 310, Training Loss: 0.032134927809238434, Testing Loss: 0.030803004279732704\n",
      "Epoch: 311, Training Loss: 0.03213285282254219, Testing Loss: 0.030801154673099518\n",
      "Epoch: 312, Training Loss: 0.03213079646229744, Testing Loss: 0.030799461528658867\n",
      "Epoch: 313, Training Loss: 0.03212876245379448, Testing Loss: 0.030797362327575684\n",
      "Epoch: 314, Training Loss: 0.032126739621162415, Testing Loss: 0.030795393511652946\n",
      "Epoch: 315, Training Loss: 0.032124750316143036, Testing Loss: 0.03079395554959774\n",
      "Epoch: 316, Training Loss: 0.03212277591228485, Testing Loss: 0.030792146921157837\n",
      "Epoch: 317, Training Loss: 0.03212081640958786, Testing Loss: 0.030790014192461967\n",
      "Epoch: 318, Training Loss: 0.03211887180805206, Testing Loss: 0.0307886004447937\n",
      "Epoch: 319, Training Loss: 0.03211694583296776, Testing Loss: 0.030786728486418724\n",
      "Epoch: 320, Training Loss: 0.03211503475904465, Testing Loss: 0.030784672126173973\n",
      "Epoch: 321, Training Loss: 0.032113153487443924, Testing Loss: 0.030783481895923615\n",
      "Epoch: 322, Training Loss: 0.0321112796664238, Testing Loss: 0.030781790614128113\n",
      "Epoch: 323, Training Loss: 0.032109420746564865, Testing Loss: 0.03077954426407814\n",
      "Epoch: 324, Training Loss: 0.03210758790373802, Testing Loss: 0.03077809512615204\n",
      "Epoch: 325, Training Loss: 0.03210575506091118, Testing Loss: 0.030777035281062126\n",
      "Epoch: 326, Training Loss: 0.03210395947098732, Testing Loss: 0.030775120481848717\n",
      "Epoch: 327, Training Loss: 0.032102152705192566, Testing Loss: 0.03077317215502262\n",
      "Epoch: 328, Training Loss: 0.0321003720164299, Testing Loss: 0.030771343037486076\n",
      "Epoch: 329, Training Loss: 0.032098617404699326, Testing Loss: 0.030769987031817436\n",
      "Epoch: 330, Training Loss: 0.03209685906767845, Testing Loss: 0.030768433585762978\n",
      "Epoch: 331, Training Loss: 0.032095134258270264, Testing Loss: 0.030766615644097328\n",
      "Epoch: 332, Training Loss: 0.03209341689944267, Testing Loss: 0.030764685943722725\n",
      "Epoch: 333, Training Loss: 0.03209172189235687, Testing Loss: 0.030763279646635056\n",
      "Epoch: 334, Training Loss: 0.03209003061056137, Testing Loss: 0.03076205588877201\n",
      "Epoch: 335, Training Loss: 0.03208836540579796, Testing Loss: 0.03076046146452427\n",
      "Epoch: 336, Training Loss: 0.032086703926324844, Testing Loss: 0.030758431181311607\n",
      "Epoch: 337, Training Loss: 0.03208506852388382, Testing Loss: 0.03075707145035267\n",
      "Epoch: 338, Training Loss: 0.03208343684673309, Testing Loss: 0.030756190419197083\n",
      "Epoch: 339, Training Loss: 0.03208183869719505, Testing Loss: 0.030754458159208298\n",
      "Epoch: 340, Training Loss: 0.032080233097076416, Testing Loss: 0.030752282589673996\n",
      "Epoch: 341, Training Loss: 0.03207865729928017, Testing Loss: 0.030750907957553864\n",
      "Epoch: 342, Training Loss: 0.032077088952064514, Testing Loss: 0.030749909579753876\n",
      "Epoch: 343, Training Loss: 0.03207554295659065, Testing Loss: 0.030748611316084862\n",
      "Epoch: 344, Training Loss: 0.03207401558756828, Testing Loss: 0.03074696660041809\n",
      "Epoch: 345, Training Loss: 0.03207249194383621, Testing Loss: 0.030745353549718857\n",
      "Epoch: 346, Training Loss: 0.032070983201265335, Testing Loss: 0.03074391558766365\n",
      "Epoch: 347, Training Loss: 0.032069481909275055, Testing Loss: 0.030742570757865906\n",
      "Epoch: 348, Training Loss: 0.03206799551844597, Testing Loss: 0.030741270631551743\n",
      "Epoch: 349, Training Loss: 0.03206652030348778, Testing Loss: 0.030740033835172653\n",
      "Epoch: 350, Training Loss: 0.03206505626440048, Testing Loss: 0.030738743022084236\n",
      "Epoch: 351, Training Loss: 0.03206360340118408, Testing Loss: 0.030737340450286865\n",
      "Epoch: 352, Training Loss: 0.032062165439128876, Testing Loss: 0.030735887587070465\n",
      "Epoch: 353, Training Loss: 0.03206075355410576, Testing Loss: 0.030734533444046974\n",
      "Epoch: 354, Training Loss: 0.03205934911966324, Testing Loss: 0.030733276158571243\n",
      "Epoch: 355, Training Loss: 0.032057955861091614, Testing Loss: 0.03073202818632126\n",
      "Epoch: 356, Training Loss: 0.03205658495426178, Testing Loss: 0.030730560421943665\n",
      "Epoch: 357, Training Loss: 0.03205522149801254, Testing Loss: 0.030729010701179504\n",
      "Epoch: 358, Training Loss: 0.0320538654923439, Testing Loss: 0.030727524310350418\n",
      "Epoch: 359, Training Loss: 0.032052524387836456, Testing Loss: 0.030726321041584015\n",
      "Epoch: 360, Training Loss: 0.032051194459199905, Testing Loss: 0.03072531148791313\n",
      "Epoch: 361, Training Loss: 0.03204986825585365, Testing Loss: 0.03072441555559635\n",
      "Epoch: 362, Training Loss: 0.03204856440424919, Testing Loss: 0.030722815543413162\n",
      "Epoch: 363, Training Loss: 0.03204725310206413, Testing Loss: 0.030721046030521393\n",
      "Epoch: 364, Training Loss: 0.032045960426330566, Testing Loss: 0.03071960248053074\n",
      "Epoch: 365, Training Loss: 0.03204469010233879, Testing Loss: 0.030718963593244553\n",
      "Epoch: 366, Training Loss: 0.03204340487718582, Testing Loss: 0.030718445777893066\n",
      "Epoch: 367, Training Loss: 0.03204217180609703, Testing Loss: 0.030716896057128906\n",
      "Epoch: 368, Training Loss: 0.03204089775681496, Testing Loss: 0.030714750289916992\n",
      "Epoch: 369, Training Loss: 0.03203970938920975, Testing Loss: 0.030713796615600586\n",
      "Epoch: 370, Training Loss: 0.03203846141695976, Testing Loss: 0.030713751912117004\n",
      "Epoch: 371, Training Loss: 0.032037265598773956, Testing Loss: 0.03071233443915844\n",
      "Epoch: 372, Training Loss: 0.03203604370355606, Testing Loss: 0.03071010485291481\n",
      "Epoch: 373, Training Loss: 0.03203486278653145, Testing Loss: 0.030709007754921913\n",
      "Epoch: 374, Training Loss: 0.03203367069363594, Testing Loss: 0.03070889599621296\n",
      "Epoch: 375, Training Loss: 0.03203248977661133, Testing Loss: 0.030707772821187973\n",
      "Epoch: 376, Training Loss: 0.03203132748603821, Testing Loss: 0.03070576675236225\n",
      "Epoch: 377, Training Loss: 0.03203015774488449, Testing Loss: 0.030704354867339134\n",
      "Epoch: 378, Training Loss: 0.03202902898192406, Testing Loss: 0.030703870579600334\n",
      "Epoch: 379, Training Loss: 0.032027848064899445, Testing Loss: 0.030703652650117874\n",
      "Epoch: 380, Training Loss: 0.0320267453789711, Testing Loss: 0.03070232644677162\n",
      "Epoch: 381, Training Loss: 0.032025597989559174, Testing Loss: 0.03070034086704254\n",
      "Epoch: 382, Training Loss: 0.03202448785305023, Testing Loss: 0.03069918416440487\n",
      "Epoch: 383, Training Loss: 0.03202338144183159, Testing Loss: 0.03069889359176159\n",
      "Epoch: 384, Training Loss: 0.03202226012945175, Testing Loss: 0.030698200687766075\n",
      "Epoch: 385, Training Loss: 0.032021183520555496, Testing Loss: 0.030696600675582886\n",
      "Epoch: 386, Training Loss: 0.032020051032304764, Testing Loss: 0.03069477714598179\n",
      "Epoch: 387, Training Loss: 0.032018981873989105, Testing Loss: 0.030693862587213516\n",
      "Epoch: 388, Training Loss: 0.03201790153980255, Testing Loss: 0.03069332428276539\n",
      "Epoch: 389, Training Loss: 0.03201683238148689, Testing Loss: 0.030692312866449356\n",
      "Epoch: 390, Training Loss: 0.03201577812433243, Testing Loss: 0.030690915882587433\n",
      "Epoch: 391, Training Loss: 0.03201473131775856, Testing Loss: 0.03068980947136879\n",
      "Epoch: 392, Training Loss: 0.03201369196176529, Testing Loss: 0.030688932165503502\n",
      "Epoch: 393, Training Loss: 0.032012660056352615, Testing Loss: 0.030688149854540825\n",
      "Epoch: 394, Training Loss: 0.03201163932681084, Testing Loss: 0.030686991289258003\n",
      "Epoch: 395, Training Loss: 0.03201061487197876, Testing Loss: 0.030685869976878166\n",
      "Epoch: 396, Training Loss: 0.03200960531830788, Testing Loss: 0.030684826895594597\n",
      "Epoch: 397, Training Loss: 0.03200859948992729, Testing Loss: 0.030683916062116623\n",
      "Epoch: 398, Training Loss: 0.032007597386837006, Testing Loss: 0.03068302758038044\n",
      "Epoch: 399, Training Loss: 0.03200659528374672, Testing Loss: 0.030681999400258064\n",
      "Epoch: 400, Training Loss: 0.03200560063123703, Testing Loss: 0.030680960044264793\n",
      "Epoch: 401, Training Loss: 0.03200462460517883, Testing Loss: 0.030679933726787567\n",
      "Epoch: 402, Training Loss: 0.032003648579120636, Testing Loss: 0.03067898191511631\n",
      "Epoch: 403, Training Loss: 0.032002683728933334, Testing Loss: 0.03067820519208908\n",
      "Epoch: 404, Training Loss: 0.03200172632932663, Testing Loss: 0.030677050352096558\n",
      "Epoch: 405, Training Loss: 0.03200077265501022, Testing Loss: 0.030676066875457764\n",
      "Epoch: 406, Training Loss: 0.031999826431274414, Testing Loss: 0.030675271525979042\n",
      "Epoch: 407, Training Loss: 0.0319988839328289, Testing Loss: 0.030674418434500694\n",
      "Epoch: 408, Training Loss: 0.03199794888496399, Testing Loss: 0.03067348711192608\n",
      "Epoch: 409, Training Loss: 0.03199701011180878, Testing Loss: 0.03067253716289997\n",
      "Epoch: 410, Training Loss: 0.03199607878923416, Testing Loss: 0.030671730637550354\n",
      "Epoch: 411, Training Loss: 0.03199515864253044, Testing Loss: 0.03067091479897499\n",
      "Epoch: 412, Training Loss: 0.03199424594640732, Testing Loss: 0.03067001700401306\n",
      "Epoch: 413, Training Loss: 0.031993329524993896, Testing Loss: 0.03066905587911606\n",
      "Epoch: 414, Training Loss: 0.03199242427945137, Testing Loss: 0.030668159946799278\n",
      "Epoch: 415, Training Loss: 0.03199153393507004, Testing Loss: 0.030667535960674286\n",
      "Epoch: 416, Training Loss: 0.0319906510412693, Testing Loss: 0.030667027458548546\n",
      "Epoch: 417, Training Loss: 0.03198978677392006, Testing Loss: 0.030666247010231018\n",
      "Epoch: 418, Training Loss: 0.031988926231861115, Testing Loss: 0.03066539205610752\n",
      "Epoch: 419, Training Loss: 0.03198806941509247, Testing Loss: 0.030664624646306038\n",
      "Epoch: 420, Training Loss: 0.031987231224775314, Testing Loss: 0.030663946643471718\n",
      "Epoch: 421, Training Loss: 0.03198638930916786, Testing Loss: 0.03066325932741165\n",
      "Epoch: 422, Training Loss: 0.031985558569431305, Testing Loss: 0.030662070959806442\n",
      "Epoch: 423, Training Loss: 0.031984735280275345, Testing Loss: 0.030661270022392273\n",
      "Epoch: 424, Training Loss: 0.03198391571640968, Testing Loss: 0.030660688877105713\n",
      "Epoch: 425, Training Loss: 0.03198309987783432, Testing Loss: 0.030659979209303856\n",
      "Epoch: 426, Training Loss: 0.03198229521512985, Testing Loss: 0.030658897012472153\n",
      "Epoch: 427, Training Loss: 0.031981486827135086, Testing Loss: 0.03065786138176918\n",
      "Epoch: 428, Training Loss: 0.03198069706559181, Testing Loss: 0.030657146126031876\n",
      "Epoch: 429, Training Loss: 0.03197989612817764, Testing Loss: 0.0306564811617136\n",
      "Epoch: 430, Training Loss: 0.03197911009192467, Testing Loss: 0.030655667185783386\n",
      "Epoch: 431, Training Loss: 0.03197833150625229, Testing Loss: 0.030654504895210266\n",
      "Epoch: 432, Training Loss: 0.03197755664587021, Testing Loss: 0.030653512105345726\n",
      "Epoch: 433, Training Loss: 0.031976789236068726, Testing Loss: 0.030652878805994987\n",
      "Epoch: 434, Training Loss: 0.031976018100976944, Testing Loss: 0.030652407556772232\n",
      "Epoch: 435, Training Loss: 0.031975261867046356, Testing Loss: 0.03065166249871254\n",
      "Epoch: 436, Training Loss: 0.03197450563311577, Testing Loss: 0.030650557950139046\n",
      "Epoch: 437, Training Loss: 0.03197374567389488, Testing Loss: 0.03064960055053234\n",
      "Epoch: 438, Training Loss: 0.03197300434112549, Testing Loss: 0.030648985877633095\n",
      "Epoch: 439, Training Loss: 0.031972259283065796, Testing Loss: 0.030648432672023773\n",
      "Epoch: 440, Training Loss: 0.031971525400877, Testing Loss: 0.030647803097963333\n",
      "Epoch: 441, Training Loss: 0.0319707989692688, Testing Loss: 0.030646946281194687\n",
      "Epoch: 442, Training Loss: 0.0319700688123703, Testing Loss: 0.03064587153494358\n",
      "Epoch: 443, Training Loss: 0.03196936845779419, Testing Loss: 0.030645623803138733\n",
      "Epoch: 444, Training Loss: 0.03196864202618599, Testing Loss: 0.030645102262496948\n",
      "Epoch: 445, Training Loss: 0.03196794539690018, Testing Loss: 0.030644230544567108\n",
      "Epoch: 446, Training Loss: 0.03196725249290466, Testing Loss: 0.03064313344657421\n",
      "Epoch: 447, Training Loss: 0.03196658194065094, Testing Loss: 0.03064267337322235\n",
      "Epoch: 448, Training Loss: 0.031965889036655426, Testing Loss: 0.030642559751868248\n",
      "Epoch: 449, Training Loss: 0.031965237110853195, Testing Loss: 0.030641978606581688\n",
      "Epoch: 450, Training Loss: 0.03196457773447037, Testing Loss: 0.03064075857400894\n",
      "Epoch: 451, Training Loss: 0.03196389600634575, Testing Loss: 0.03063965030014515\n",
      "Epoch: 452, Training Loss: 0.03196327015757561, Testing Loss: 0.03063937835395336\n",
      "Epoch: 453, Training Loss: 0.03196259215474129, Testing Loss: 0.03063940815627575\n",
      "Epoch: 454, Training Loss: 0.03196198493242264, Testing Loss: 0.030638378113508224\n",
      "Epoch: 455, Training Loss: 0.03196130692958832, Testing Loss: 0.030637102201581\n",
      "Epoch: 456, Training Loss: 0.03196069970726967, Testing Loss: 0.030636699870228767\n",
      "Epoch: 457, Training Loss: 0.03196004778146744, Testing Loss: 0.03063669241964817\n",
      "Epoch: 458, Training Loss: 0.03195944055914879, Testing Loss: 0.03063586726784706\n",
      "Epoch: 459, Training Loss: 0.03195881098508835, Testing Loss: 0.03063461370766163\n",
      "Epoch: 460, Training Loss: 0.031958211213350296, Testing Loss: 0.030633943155407906\n",
      "Epoch: 461, Training Loss: 0.031957611441612244, Testing Loss: 0.030633671209216118\n",
      "Epoch: 462, Training Loss: 0.0319569893181324, Testing Loss: 0.030633628368377686\n",
      "Epoch: 463, Training Loss: 0.031956419348716736, Testing Loss: 0.030632730573415756\n",
      "Epoch: 464, Training Loss: 0.03195580840110779, Testing Loss: 0.030631734058260918\n",
      "Epoch: 465, Training Loss: 0.03195523843169212, Testing Loss: 0.03063100576400757\n",
      "Epoch: 466, Training Loss: 0.03195465728640556, Testing Loss: 0.030630633234977722\n",
      "Epoch: 467, Training Loss: 0.031954076141119, Testing Loss: 0.030630528926849365\n",
      "Epoch: 468, Training Loss: 0.03195350244641304, Testing Loss: 0.030629752203822136\n",
      "Epoch: 469, Training Loss: 0.03195292502641678, Testing Loss: 0.030628889799118042\n",
      "Epoch: 470, Training Loss: 0.03195236250758171, Testing Loss: 0.030628329142928123\n",
      "Epoch: 471, Training Loss: 0.03195180371403694, Testing Loss: 0.03062795102596283\n",
      "Epoch: 472, Training Loss: 0.03195124864578247, Testing Loss: 0.030627291649580002\n",
      "Epoch: 473, Training Loss: 0.0319506973028183, Testing Loss: 0.030626488849520683\n",
      "Epoch: 474, Training Loss: 0.031950145959854126, Testing Loss: 0.03062557615339756\n",
      "Epoch: 475, Training Loss: 0.03194960579276085, Testing Loss: 0.030624818056821823\n",
      "Epoch: 476, Training Loss: 0.03194906935095787, Testing Loss: 0.030624302104115486\n",
      "Epoch: 477, Training Loss: 0.031948525458574295, Testing Loss: 0.030623963102698326\n",
      "Epoch: 478, Training Loss: 0.031947989016771317, Testing Loss: 0.030623693019151688\n",
      "Epoch: 479, Training Loss: 0.031947460025548935, Testing Loss: 0.03062337450683117\n",
      "Epoch: 480, Training Loss: 0.03194693848490715, Testing Loss: 0.03062259592115879\n",
      "Epoch: 481, Training Loss: 0.03194640949368477, Testing Loss: 0.030621863901615143\n",
      "Epoch: 482, Training Loss: 0.03194589167833328, Testing Loss: 0.03062150627374649\n",
      "Epoch: 483, Training Loss: 0.031945373862981796, Testing Loss: 0.03062136285007\n",
      "Epoch: 484, Training Loss: 0.0319448783993721, Testing Loss: 0.030620522797107697\n",
      "Epoch: 485, Training Loss: 0.031944360584020615, Testing Loss: 0.030619623139500618\n",
      "Epoch: 486, Training Loss: 0.03194385766983032, Testing Loss: 0.030619166791439056\n",
      "Epoch: 487, Training Loss: 0.03194334730505943, Testing Loss: 0.03061886504292488\n",
      "Epoch: 488, Training Loss: 0.03194285184144974, Testing Loss: 0.030618121847510338\n",
      "Epoch: 489, Training Loss: 0.031942348927259445, Testing Loss: 0.03061731718480587\n",
      "Epoch: 490, Training Loss: 0.031941864639520645, Testing Loss: 0.030616918578743935\n",
      "Epoch: 491, Training Loss: 0.03194137662649155, Testing Loss: 0.030616508796811104\n",
      "Epoch: 492, Training Loss: 0.031940896064043045, Testing Loss: 0.030615607276558876\n",
      "Epoch: 493, Training Loss: 0.03194041922688484, Testing Loss: 0.03061504475772381\n",
      "Epoch: 494, Training Loss: 0.03193994238972664, Testing Loss: 0.030614931136369705\n",
      "Epoch: 495, Training Loss: 0.031939465552568436, Testing Loss: 0.03061433881521225\n",
      "Epoch: 496, Training Loss: 0.03193899244070053, Testing Loss: 0.03061358816921711\n",
      "Epoch: 497, Training Loss: 0.03193853050470352, Testing Loss: 0.03061319701373577\n",
      "Epoch: 498, Training Loss: 0.031938064843416214, Testing Loss: 0.03061300329864025\n",
      "Epoch: 499, Training Loss: 0.0319376066327095, Testing Loss: 0.030612586066126823\n"
     ]
    }
   ],
   "source": [
    "loss_history, loss_history_test = train_and_test(net, x_train_tensor, y_train_tensor, x_test_tensor, y_test_tensor, epochs=500)\n",
    "y_hat = evaluate(net, x_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history, label='Training loss', color='blue')\n",
    "plt.plot(loss_history_test, label='Testing loss', color='red')\n",
    "plt.title('Loss per epoch');\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnq-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
