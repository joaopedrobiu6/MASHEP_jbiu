{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1717847643531,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"gokGmHeEWd3h","outputId":"f219b1fe-b5ac-4438-bb35-d3be6338c1bf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","from sklearn.metrics import roc_curve, roc_auc_score\n","from sklearn.ensemble import RandomForestClassifier\n","tf.random.set_seed(2)\n","\n","import pyhf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create output directory\n","OUT_DIR = \"./MachineLearningRF/\"\n","os.makedirs(OUT_DIR, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2562,"status":"ok","timestamp":1717847649047,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"NDKdxL2OWd3k"},"outputs":[],"source":["# Load the dataframe with MC data and their labels (signal = 1, background = 0)\n","dataframe = pd.read_csv('dataframe.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1717847649048,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"lUDd005QWd3k","outputId":"7a3ce8d8-5e5a-48dc-97ea-609b769fe45d"},"outputs":[],"source":["# Create train weights such that the sum of train weights for signal and background is equal (ratio bkg/sig = 1:1)\n","dataframe['train_weight'] = 1.\n","dataframe.loc[dataframe.query('label==1').index,'train_weight'] = (dataframe.loc[dataframe.query('label==1').index,'scaleweight']) / dataframe.loc[dataframe.query('label==1').index,'scaleweight'].sum()\n","dataframe.loc[dataframe.query('label==0').index,'train_weight'] = dataframe.loc[dataframe.query('label==0').index,'scaleweight'] / dataframe.loc[dataframe.query('label==0').index,'scaleweight'].sum()\n","\n","sum_w_sig = dataframe.query('label==0')['train_weight'].sum()\n","sum_w_bkg = dataframe.query('label==1')['train_weight'].sum()\n","print(f'Sum of weights for training Signal {sum_w_sig:.3} and Background {sum_w_bkg:.3}')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":222,"status":"ok","timestamp":1717847649048,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"_W8MaOerWd3k"},"outputs":[],"source":["# Define the features and weights to be used for training\n","train_features = ['etmiss', 'mtw', 'leadleptPt', 'leadleptEta',\n","       'leadleptE', 'leadleptPhi', 'Q_leadlep', 'n_TopLRjets',\n","       'leadTopLRjet_pt', 'leadTopLRjet_eta', 'leadTopLRjet_phi',\n","       'leadTopLRjet_m', 'leadTopLRjet_Tau32', 'n_jets', 'leadjet_pt',\n","       'leadjet_eta', 'n_bjets', 'leadbjet_pt', 'leadbjet_eta', 'ttbarMLR']\n","weights = [\"scaleweight\", \"train_weight\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":198,"status":"ok","timestamp":1717847649048,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"cE8LHZVOWd3l"},"outputs":[],"source":["# Split the dataframe into train, validation and test samples, and scale the features\n","x_train, x_val, y_train, y_val, w_train_full, w_val_full = train_test_split(dataframe[train_features].values, dataframe['label'].values, dataframe[weights].values, train_size = 1/3, random_state = 64)\n","x_val, x_test, y_val, y_test, w_val_full, w_test_full = train_test_split(x_val, y_val, w_val_full, test_size=1/2, random_state = 64)\n","\n","scaler = StandardScaler()\n","scaler.fit(x_train)\n","\n","x_train = scaler.transform(x_train)\n","x_val = scaler.transform(x_val)\n","x_test = scaler.transform(x_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Verify that the ratio of signal to background in the training, validation and test sets is equal to 1\n","print(f\"Ratio of signal to background in training set: {w_train_full[:, 1][y_train==1].sum() / w_train_full[:, 1][y_train==0].sum():.2f}\")\n","print(f\"Ratio of signal to background in validation set: {w_val_full[:, 1][y_val==1].sum() / w_val_full[:, 1][y_val==0].sum():.2f}\")\n","print(f\"Ratio of signal to background in test set: {w_test_full[:, 1][y_test==1].sum() / w_test_full[:, 1][y_test==0].sum():.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":171,"status":"ok","timestamp":1717847649048,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"cpdDHJr8Wd3l"},"outputs":[],"source":["# Separate the weights into scaleweights to use in the fit, and train weights to use in the training\n","def weight_separation(w):\n","    scaleweights = w[:, 0]\n","    train = w[:, 1]\n","    return scaleweights, train\n","\n","\"\"\"\n","w_<set>_scale are the scaleweights to be used in the fit\n","w_<set> are the train weights to be used in the training\n","\"\"\"\n","w_train_scale, w_train = weight_separation(w_train_full)\n","w_val_scale, w_val = weight_separation(w_val_full)\n","w_test_scale, w_test = weight_separation(w_test_full)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier = RandomForestClassifier(n_estimators = 200)\n","classifier.fit(x_train, y_train, sample_weight=w_train)\n","score = classifier.score(x_test, y_test, sample_weight=w_test)\n","\n","pred_test = classifier.predict_proba(x_test)\n","pred_val = classifier.predict_proba(x_val)\n","\n","sig_pred_test = pred_test[:, 1]\n","sig_pred_val = pred_val[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bins = plt.hist(sig_pred_test[np.where(y_test==0)], bins=20, density=True, histtype='step', color='blue', label='bkg train', range=(0, 1))\n","# plt.hist(sig_pred_val[np.where(y_val==0)], bins=bins[1], density=True, histtype='step', color='red', label='bkg validation', range=(0, 1))\n","plt.hist(sig_pred_test[np.where(y_train==1)], bins=bins[1], density=True, histtype='step', color='orange', label='sig train', range=(0, 1))\n","# plt.hist(sig_pred_val[np.where(y_val==1)], bins=bins[1], density=True, histtype='step', color='green', label='sig validation', range=(0, 1))\n","plt.xlabel('RF output')\n","plt.legend()\n","plt.show()\n","\n","fpr, tpr, _ = roc_curve(y_test, sig_pred_test)\n","auc = roc_auc_score(y_test, sig_pred_test)\n","plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.title(\"ROC curve for Random Forest classifier\")\n","plt.legend()\n","plt.savefig(OUT_DIR + \"ROC_curve.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1717847814323,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"YR_BmI9MWd3m","outputId":"b1b2c63d-bb1e-485c-af9b-7100b4ffab50"},"outputs":[],"source":["# Plot the histogram of the NN output for the test set\n","bin = plt.hist(sig_pred_test[y_test==0], bins=20, density=False, histtype='step', color='blue', label='bkg test', weights=w_test_scale[y_test==0])\n","plt.hist(sig_pred_test[y_test==1], bins=bin[1], density=False, histtype='step', color='red', label='sig test', weights=w_test_scale[y_test==1])\n","\n","plt.xlabel('NN output')\n","plt.ylabel('Counts')\n","plt.title('NN output for test set')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_test_output.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"elapsed":1308,"status":"ok","timestamp":1717847815543,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"GIYUbZs5Wd3m","outputId":"1ce12daf-6b2c-4fc2-da12-b31674873bf9"},"outputs":[],"source":["# # Plot the histogram of the NN output for the training and validation sets\n","# bins = plt.hist(y_train_pred[y_train==1], bins=20, density=False, histtype='step', color='blue', label='sig train', weights=w_train[y_train==1])\n","# plt.hist(y_val_pred[y_val==1], bins=bins[1], density=False, histtype='step', color='red', label='sig val', weights=w_val[y_val==1])\n","# bins = plt.hist(y_train_pred[y_train==0], bins=bins[1], density=False, histtype='step', color='green', label='bkg train', weights=w_train[y_train==0])\n","# plt.hist(y_val_pred[y_val==0], bins=bins[1], density=False, histtype='step', color='orange', label='bkg val', weights=w_val[y_val==0])\n","\n","# plt.xlabel('NN output')\n","# plt.ylabel('Counts')\n","# plt.title('NN output for training and validation sets')\n","# plt.legend()\n","# plt.savefig(OUT_DIR + \"NN_train_val_output.png\")\n","# plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":462,"status":"ok","timestamp":1717847815543,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"Gp0HHteDWd3m","outputId":"d40926c0-85e2-40c7-a5cb-80f148468a82"},"outputs":[],"source":["# ROC curve for the test set\n","fpr, tpr, thresholds = roc_curve(y_test, sig_pred_test)\n","auc = roc_auc_score(y_test, sig_pred_test)\n","plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n","plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n","plt.title('ROC curve')\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"ROC_curve.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":435,"status":"ok","timestamp":1717847815543,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"9ZwMB2ofWd3m"},"outputs":[],"source":["# Create dataframe of the predictions and its respective scaleweights\n","def join_y_w(y, w):\n","    df = pd.DataFrame({'y_pred': y, 'scaleweights': w})\n","    return df\n","\n","# df_train = join_y_w(y_train_pred[:, 0], w_train_scale[:])\n","# df_val = join_y_w(y_val_pred[:, 0], w_val_scale[:])\n","df_test = join_y_w(sig_pred_test, w_test_scale[:]*3) # Multiply by 3 to account for the 1/3 of the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":439},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1717847815543,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"DHEODbliWd3n","outputId":"c02901a0-fac2-4f13-dff9-e6fa162502c7"},"outputs":[],"source":["# Import the measured data\n","data = pd.read_csv('Output_ZPrimeBoostedAnalysis/data.csv', delimiter= \" \")\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":334,"status":"ok","timestamp":1717847815543,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"bP_HpD7ZWd3n"},"outputs":[],"source":["# Apply the same scaling to the measured data and get the weights\n","x_data = scaler.transform(data[train_features].values)\n","w_data = data[\"weight\"].values\n","\n","# Compute the predictions for the measured data\n","y_data_pred = classifier.predict_proba(x_data)[:, 1]\n","\n","# Create dataframe of the predictions and its respective weights\n","df_data = join_y_w(y_data_pred, w_data[:])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1717847816178,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"NpgS8vxoWd3n","outputId":"02ca9a79-d937-4556-e297-f35fdbe552b1"},"outputs":[],"source":["# Plot the histogram of the NN output for the measured data and the test set\n","bins = plt.hist(df_data['y_pred'], bins=20, histtype='step', color='blue', label='Measured Data', weights=df_data['scaleweights'])\n","plt.hist(df_test['y_pred'], bins=bins[1], histtype='step', color='red', label='Test Set Output', weights=df_test['scaleweights'])\n","plt.xlabel('NN output')\n","plt.ylabel('Events')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_data_test_output.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":100,"status":"ok","timestamp":1717847816178,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"yIYw53MsWd3o"},"outputs":[],"source":["# Create the distribution of the measured data and the test set signal and background NN output\n","N, bins = np.histogram(df_data['y_pred'], bins=20, weights=df_data['scaleweights'])\n","\n","B = np.histogram(df_test['y_pred'][y_test==0], bins=bins, weights=df_test['scaleweights'][y_test==0])[0]\n","S = np.histogram(df_test['y_pred'][y_test==1], bins=bins, weights=df_test['scaleweights'][y_test==1])[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Select the bins where there is less signal (the background is more pure) to use in the fit to discover the background normalization factor epsilon\n","N2 = N[:9]\n","B2 = B[:9]\n","S2 = S[:9]\n","\n","#plot\n","plt.hist(bins[:9], bins, weights=N2, histtype='step', color='blue', label='data')\n","plt.hist(bins[:9], bins, weights=B2, histtype='step', color='red', label='bkg')\n","plt.hist(bins[:9], bins, weights=S2, histtype='step', color='green', label='sig')\n","plt.xlabel('NN output')\n","plt.ylabel('Events')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_bkg_cut.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the NN output and its respective type and weights to a csv file (measured data, test set, train set, validation set)\n","dataframe_to_save1 = pd.DataFrame({'Type': 'Observed', 'NN output': df_data['y_pred'], \"weights\": df_data['scaleweights']})\n","dataframe_to_save2 = pd.DataFrame({'Type': 'Test Bkg', 'NN output': df_test['y_pred'][y_test==0], \"weights\": (df_test['scaleweights'][y_test==0])/3})\n","dataframe_to_save3 = pd.DataFrame({'Type': 'Test Sig', 'NN output': df_test['y_pred'][y_test==1], \"weights\": (df_test['scaleweights'][y_test==1])/3})\n","dataframe_to_save4 = pd.DataFrame({'Type': 'Train Bkg', 'NN output': df_train['y_pred'][y_train==0], \"weights\": (df_test['scaleweights'][y_train==0])})\n","dataframe_to_save5 = pd.DataFrame({'Type': 'Train Sig', 'NN output': df_train['y_pred'][y_train==1], \"weights\": (df_test['scaleweights'][y_train==1])})\n","dataframe_to_save6 = pd.DataFrame({'Type': 'Val Bkg', 'NN output': df_val['y_pred'][y_val==0], \"weights\": (df_test['scaleweights'][y_val==0])})\n","dataframe_to_save7 = pd.DataFrame({'Type': 'Val Sig', 'NN output': df_val['y_pred'][y_val==1], \"weights\": (df_test['scaleweights'][y_val==1])})\n","\n","dataframe_to_save = pd.concat([dataframe_to_save1, dataframe_to_save2, dataframe_to_save3, dataframe_to_save4, dataframe_to_save5, dataframe_to_save6, dataframe_to_save7])\n","dataframe_to_save.to_csv(OUT_DIR + \"NN_output.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataframe_to_save"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":696,"status":"ok","timestamp":1717847816805,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"sXcUtEa7Wd3o","outputId":"99cb5863-9849-43f3-b0d4-02b0d0f192a1"},"outputs":[],"source":["# Plot the distribution of the measured data, the signal and background\n","plt.hist(bins[:-1], bins, weights=N, histtype='step', color='blue', label='data')\n","plt.hist(bins[:-1], bins, weights=B, histtype='step', color='green', label='bkg')\n","plt.hist(bins[:-1], bins, weights=S, histtype='step', color='red', label='sig')\n","\n","plt.xlabel('NN output')\n","plt.ylabel('Events')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_NSB.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the model to the data to obtain the background normalization factor epsilon, using the bins with less signal\n","\n","model_spec_discovery = {'channels': [{'name': 'singlechannel',\n","              'samples': [\n","              {'name': 'signal','data': S2.tolist(),\n","               'modifiers': []},\n","              {'name': 'bkg1','data': B2.tolist(),\n","               'modifiers': [{'data': None, 'name': 'epsilon', 'type': 'normfactor'}]},\n","              ]\n","              }],\n","              \"observations\": [{ \"name\": \"singlechannel\", \"data\": N2.tolist() }],\n","              \"measurements\": [{ \"name\": \"Measurement\", \"config\": {\"poi\": 'epsilon', \"parameters\": []}}],\n","\n","              \"version\": \"1.0.0\",\n","}\n","\n","workspace_discovery = pyhf.Workspace(model_spec_discovery)\n","model_discovery = workspace_discovery.model()\n","\n","print(\"## Model\")\n","print(f\"  channels: {model_discovery.config.channels}\")\n","print(f\"     nbins: {model_discovery.config.channel_nbins}\")\n","print(f\"   samples: {model_discovery.config.samples}\")\n","print(f\" modifiers: {model_discovery.config.modifiers}\")\n","print(f\"parameters: {model_discovery.config.parameters}\")\n","print(f\"par. order: {model_discovery.config.par_order}\")\n","\n","print(\"\\n## Model parameters\")\n","print(f'   default: {model_discovery.config.suggested_init()}')\n","print(f'    bounds: {model_discovery.config.suggested_bounds()}')\n","\n","data = N2.tolist() + model_discovery.config.auxdata\n","\n","test_stat = \"q0\"\n","test_poi = 0.\n","\n","best_pars = pyhf.infer.mle.fit(data=data, pdf=model_discovery)\n","print(\"\\nBest fit parameters:\")\n","print(f\"  epsilon: {best_pars[0]:.2e}\")\n","\n","epsilon = best_pars[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the distribution of the measured data, the signal and normalized background\n","B3 = epsilon * B\n","\n","plt.hist(bins[:-1], bins, weights=N, histtype='step', color='blue', label='measured data')\n","plt.hist(bins[:-1], bins, weights=B3, histtype='step', color='green', label=r'bkg * $\\epsilon$ = bkg * %.2e' % epsilon)\n","plt.hist(bins[:-1], bins, weights=S, histtype='step', color='red', label='sig')\n","plt.xlabel('NN output')\n","plt.ylabel('Events')\n","plt.legend()\n","plt.savefig(OUT_DIR + \"NN_NSB_discovery.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Exclusion Fit to obtain the upper limit on the signal strength\n","model_spec = {'channels': [{'name': 'singlechannel',\n","              'samples': [\n","              {'name': 'signal','data': S.tolist(),\n","               'modifiers': [{'data': None, 'name': 'mu', 'type': 'normfactor'}]},\n","              {'name': 'bkg1','data': B.tolist(),\n","               'modifiers': []},\n","              ]\n","              }],\n","              \"observations\": [{ \"name\": \"singlechannel\", \"data\": N.tolist() }],\n","              \"measurements\": [{ \"name\": \"Measurement\", \"config\": {\"poi\": 'mu', \"parameters\": []}}],\n","\n","              \"version\": \"1.0.0\",\n","}\n","\n","workspace = pyhf.Workspace(model_spec)\n","model = workspace.model()\n","\n","print(\"## Model\")\n","print(f\"  channels: {model.config.channels}\")\n","print(f\"     nbins: {model.config.channel_nbins}\")\n","print(f\"   samples: {model.config.samples}\")\n","print(f\" modifiers: {model.config.modifiers}\")\n","print(f\"parameters: {model.config.parameters}\")\n","print(f\"par. order: {model.config.par_order}\")\n","\n","print(\"\\n## Model parameters\")\n","print(f'   default: {model.config.suggested_init()}')\n","print(f'    bounds: {model.config.suggested_bounds()}')\n","\n","data = N.tolist() + model.config.auxdata\n","\n","test_stat = \"qtilde\"\n","test_poi = 1.\n","\n","# Vary mu values\n","poi_values = np.linspace(0.01, 5, 500)\n","obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(data, model, poi_values, level=0.05, return_results=True)\n","print(f\"\\nObserved μ upper limit (obs): {obs_limit:.3f}, Expected μ upper limit {exp_limits[2]:.3f}\")\n","\n","# CLs_obs, CLs_exp = pyhf.infer.hypotest(test_poi, data, model, test_stat=test_stat, return_expected=True)\n","# print(f\"Observed CL_s: {CLs_obs}, Expected CL_s: {CLs_exp} \")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3164,"status":"ok","timestamp":1717847819890,"user":{"displayName":"João Pedro Ferreira Biu","userId":"02334244759647496794"},"user_tz":-60},"id":"k8NSvWFMWd3o","outputId":"fd6a5259-f935-4668-ba58-329dab18397b"},"outputs":[],"source":["# Exclusion Fit to obtain the upper limit on the signal strength\n","model_spec_norm = {'channels': [{'name': 'singlechannel',\n","              'samples': [\n","              {'name': 'signal','data': S.tolist(),\n","               'modifiers': [{'data': None, 'name': 'mu', 'type': 'normfactor'}]},\n","              {'name': 'bkg1','data': B3.tolist(),\n","               'modifiers': []},\n","              ]\n","              }],\n","              \"observations\": [{ \"name\": \"singlechannel\", \"data\": N.tolist() }],\n","              \"measurements\": [{ \"name\": \"Measurement\", \"config\": {\"poi\": 'mu', \"parameters\": []}}],\n","\n","              \"version\": \"1.0.0\",\n","}\n","\n","workspace = pyhf.Workspace(model_spec_norm)\n","model = workspace.model()\n","\n","print(\"## Model\")\n","print(f\"  channels: {model.config.channels}\")\n","print(f\"     nbins: {model.config.channel_nbins}\")\n","print(f\"   samples: {model.config.samples}\")\n","print(f\" modifiers: {model.config.modifiers}\")\n","print(f\"parameters: {model.config.parameters}\")\n","print(f\"par. order: {model.config.par_order}\")\n","\n","print(\"\\n## Model parameters\")\n","print(f'   default: {model.config.suggested_init()}')\n","print(f'    bounds: {model.config.suggested_bounds()}')\n","\n","data = N.tolist() + model.config.auxdata\n","\n","test_stat = \"qtilde\"\n","test_poi = 1.\n","\n","# Vary mu values\n","poi_values = np.linspace(0.01, 5, 500)\n","obs_limit_norm, exp_limits_norm, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(data, model, poi_values, level=0.05, return_results=True)\n","print(f\"\\nObserved μ upper limit (obs): {obs_limit_norm:.3f}, Expected μ upper limit {exp_limits_norm[2]:.3f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save result to txt\n","with open(OUT_DIR + \"results.txt\", \"w\") as f:\n","    f.write(f\"Value for epsilon (background normalization factor): {epsilon}\\n\")\n","    f.write(f\"Observed μ upper limit (obs) with epsilon: {obs_limit_norm}\\n\")\n","    f.write(f\"Expected μ upper limit (exp) with epsilon: {exp_limits_norm[2]}\\n\\n\\n\")\n","    f.write(f\"Observed μ upper limit (obs): {obs_limit}\\n\")\n","    f.write(f\"Expected μ upper limit (exp): {exp_limits[2]}\\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
