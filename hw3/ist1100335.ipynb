{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":634,"status":"ok","timestamp":1716315044919,"user":{"displayName":"Patrícia Conde Muíño","userId":"14614533482720646196"},"user_tz":-60},"id":"a2MHBUYxXjIn"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import math\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"pT9s-DUQJ43b"},"source":["# Classification problem with HWW ATLAS Open data using Machine Learning\n","\n","In this exercise you are given 3 files with tabular data split by 3 sets, respectively:\n","\n"," * Set of events for training the ML classifier (data_train.csv)\n"," * Set of events for validating the ML classifier (data_val.csv)\n"," * Set of events for testing the ML classifier (data_test.csv)\n","\n","The data content is the same used for the ML class, i.e.\n","\n"," * there is a number of observables (mLL,pTLL, etc...);\n"," * a label indicating the true class of each event (0=background, 1=signal) included as a variable in the data frame;\n"," * an event weight to be used for training (\"train_weight\")."]},{"cell_type":"markdown","metadata":{"id":"mQydZDqE39bj"},"source":["## Load and inspect data\n","\n","* Open data files\n","* Inspect contents\n","* Plot variables"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":456},"executionInfo":{"elapsed":44858,"status":"ok","timestamp":1716315092540,"user":{"displayName":"Patrícia Conde Muíño","userId":"14614533482720646196"},"user_tz":-60},"id":"MKIw-3mZXuhw","outputId":"0b606d7d-421b-4425-ad54-42f205b4c037"},"outputs":[],"source":["data_train = pd.read_csv(\"https://www.lip.pt/~rute/MASimHEP/HWW_csv_prep/data_train.csv\")\n","data_val   = pd.read_csv(\"https://www.lip.pt/~rute/MASimHEP/HWW_csv_prep/data_val.csv\")\n","data_test  = pd.read_csv(\"https://www.lip.pt/~rute/MASimHEP/HWW_csv_prep/data_test.csv\")\n","\n","data_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655222309269,"user":{"displayName":"Rute Pedro","userId":"18120937919196533278"},"user_tz":-60},"id":"s9LOHUu3c6Sg","outputId":"df640a94-7dfe-46c1-d7ec-ef2d0dd75274"},"outputs":[],"source":["features = data_train.columns\n","features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3701,"status":"ok","timestamp":1655222312967,"user":{"displayName":"Rute Pedro","userId":"18120937919196533278"},"user_tz":-60},"id":"v2WH1y00c0eS","outputId":"364a224d-c6aa-4050-dd21-a97adb41bb5e"},"outputs":[],"source":["plot_features = ['mLL', 'ptLL', 'dPhi_LL', 'dPhiLLmet', 'MET', 'mt',\n","                 'Lepton1_Pt', 'Lepton1_Eta', 'Lepton1_E', 'Lepton1_Phi',\n","                 'Lepton2_Pt', 'Lepton2_Eta', 'Lepton2_E', 'Lepton2_Phi']\n","\n","for feat in plot_features:\n","\n","    #(We are filtering data signal out of the 99% quantile to get the bulk of the distribution and obtain decent axes limits...)\n","    bins = plt.hist(data_train.query(f'({feat} < {feat}.quantile(.99)) & (process==\"signal\")')[feat].values, bins=20, density=True, histtype='step', color='red', label='signal')\n","    plt.hist(data_train.query('process==\"ttbar\"')[feat].values, bins=bins[1], density=True, histtype='step', color='darkorange', label='ttbar')\n","    plt.hist(data_train.query('process==\"diboson\"')[feat].values, bins=bins[1], density=True, histtype='step', color='darkviolet', label='diboson')\n","\n","    plt.xlabel(feat)\n","    plt.ylabel('Density \\(a.u\\)')\n","    plt.legend()\n","    plt.yscale(\"log\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"MTJv3In18ioG"},"source":["## Data pre-processing for training\n","\n","* Define training 'features'\n","* Construct the inputs (x_train,x_val), weights (w_train,w_val) and target values (y_train,y_val) to train the classifier (in this step you obtain the exact same arrays as used in the ML class)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_luoBzLYcvTG"},"outputs":[],"source":["train_features = ['mLL', 'ptLL', 'dPhi_LL', 'dPhiLLmet', 'MET', 'mt', 'goodjet_n',\n","                 'goodbjet_n', 'Lepton1_Pt', 'Lepton1_Eta', 'Lepton1_E', 'Lepton1_Phi',\n","                 'Lepton1_charge', 'Lepton1_type', 'Lepton2_Pt', 'Lepton2_Eta',\n","                 'Lepton2_E', 'Lepton2_Phi', 'Lepton2_charge', 'Lepton2_type']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":347,"status":"ok","timestamp":1655222313310,"user":{"displayName":"Rute Pedro","userId":"18120937919196533278"},"user_tz":-60},"id":"1gKL5HhV2w39","outputId":"2d09adaa-2725-4361-9905-105b8385dba0"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","scaler.fit(data_train[train_features].values)\n","\n","x_train = scaler.transform(data_train[train_features].values)\n","y_train = data_train['label'].values\n","w_train = data_train['train_weight'].values\n","\n","x_val = scaler.transform(data_val[train_features].values)\n","y_val = data_val['label'].values\n","w_val = data_val['train_weight'].values\n","\n","x_test = scaler.transform(data_test[train_features].values)\n","y_test = data_test['label'].values\n","w_test = data_test['train_weight'].values\n","\n","print(f'Train sample x {len(x_train)} events (y {len(y_train)} events)')\n","print(f'Val   sample x {len(x_val)} events (y {len(y_val)} events)')\n","print(f'Test  sample x {len(x_test)} events (y {len(y_test)} events)')"]},{"cell_type":"markdown","metadata":{"id":"kDQO9zkh9M7r"},"source":["# Exercise 1: DNN - Deep Neural Network Classifier\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ebGThq5CLF_o"},"outputs":[],"source":["from sklearn.metrics import roc_curve, roc_auc_score\n","import tensorflow as tf\n","\n","from keras.callbacks import EarlyStopping\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras import Input\n","from tensorflow.keras.optimizers import Adam\n","tf.random.set_seed(2)"]},{"cell_type":"markdown","metadata":{"id":"P3G8tl1oTVei"},"source":["## 1.a) DNN Training\n","\n","Train a DNN with the hyperparameters below:\n","\n","* 3 layers with 80, 95, 70 nodes respectively\n","* Learning rate of 8e-4\n","* 100 training epochs and Early Stopping with patience 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQr5iwBEkb_6"},"outputs":[],"source":["# your answer\n","print(x_train.shape)\n","model = Sequential([Input(shape=(20,))])\n","\n","# Input and Hidden layers\n","\n","model.add(Dense(80, activation='relu'))\n","model.add(Dense(95, activation='relu'))\n","model.add(Dense(70, activation='relu'))\n","\n","# Output layer\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=8e-4), weighted_metrics=['accuracy'])\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history = model.fit(x_train, y_train, sample_weight=w_train, validation_data=(x_val, y_val, w_val), epochs=100, batch_size=1024, callbacks=[EarlyStopping(patience=4)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='validation')\n","plt.title('Model loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='validation')\n","plt.title('Model accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_train_pred = model.predict(x_train)\n","y_val_pred = model.predict(x_val)\n","\n","bin = plt.hist(y_train_pred, bins=100, density=True, histtype='step', color='blue', label='train')\n","plt.hist(y_val_pred, bins=bin[1], density=True, histtype='step', color='red', label='validation')\n","\n","plt.xlabel('NN output')\n","plt.ylabel('Density')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"ASYoDDDT9ytS"},"source":["## 1.b) Evaluate the DNN\n","\n","* Plot the DNN output for signal and background and the Receiver Operating Characteristic (ROC) curve.\n","* What is the ROC Area Under the Curve (AUC)?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hPVQT_E2tbw"},"outputs":[],"source":["# your answer\n","bins = plt.hist(y_train_pred[y_train==1], bins=100, density=True, histtype='step', color='blue', label='signal train')\n","plt.hist(y_val_pred[y_val==1], bins=bins[1], density=True, histtype='step', color='green', label='signal validation')\n","plt.hist(y_train_pred[y_train==0], bins=bins[1], density=True, histtype='step', color='orange', label='bkg train')\n","plt.hist(y_val_pred[y_val==0], bins=bins[1], density=True, histtype='step', color='red', label='bkg validation')\n","\n","plt.xlabel('NN output')\n","plt.legend()\n","plt.show()\n","\n","# RPC curve\n","fpr, tpr, _ = roc_curve(y_val, y_val_pred)\n","auc = roc_auc_score(y_val, y_val_pred)\n","plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ns5QOAb5YiXt"},"source":["# Exercise 2: RF - Random Forest Classifier\n"]},{"cell_type":"markdown","metadata":{"id":"yWa1l_DebqGe"},"source":["## 2.a) RF Training\n","\n","Train a Random Forest Classifier with the hyperparameters below:\n","\n","* Maximum depth of 3\n","* 40 decision trees (estimators)\n","\n","**HINT:** The method to retreive the class prediction from the classifier for an input set of data x\n","\n","```\n","predict_proba(x)\n","```\n","returns the probabilities of classification according to each class (0 or 1), such that\n","\n","```\n","predict_proba(x)[:,0] # class-0 probability\n","predict_proba(x)[:,1] # class-1 probability\n","```\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1vA2Uk_Y6S1"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYheJh9_71uI"},"outputs":[],"source":["# your answer\n","classifier = RandomForestClassifier(max_depth=3, n_estimators = 40)\n","classifier.fit(x_train, y_train, sample_weight=w_train)\n","score = classifier.score(x_test, y_test, sample_weight=w_test)\n","\n","pred_train = classifier.predict_proba(x_train)\n","pred_val = classifier.predict_proba(x_val)\n","\n","sig_pred_train = pred_train[:, 1]\n","sig_pred_val = pred_val[:,1]"]},{"cell_type":"markdown","metadata":{"id":"CjRyHBCLcc0x"},"source":["## 2.b) Evaluate the RF\n","\n","* Plot the classifier output for signal and background and the Receiver Operating Characteristic (ROC) curve.\n","* What is the ROC Area Under the Curve (AUC)?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IPUYpQ3Z1wL"},"outputs":[],"source":["# your answer\n","bins = plt.hist(sig_pred_train[np.where(y_train==0)], bins=100, density=True, histtype='step', color='blue', label='bkg train', range=(0, 1))\n","plt.hist(sig_pred_val[np.where(y_val==0)], bins=bins[1], density=True, histtype='step', color='red', label='bkg validation', range=(0, 1))\n","plt.hist(sig_pred_train[np.where(y_train==1)], bins=bins[1], density=True, histtype='step', color='orange', label='sig train', range=(0, 1))\n","plt.hist(sig_pred_val[np.where(y_val==1)], bins=bins[1], density=True, histtype='step', color='green', label='sig validation', range=(0, 1))\n","plt.xlabel('RF output')\n","plt.legend()\n","plt.show()\n","\n","fpr, tpr, _ = roc_curve(y_val, sig_pred_val)\n","auc = roc_auc_score(y_val, sig_pred_val)\n","plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yZ_2IHCscp4K"},"source":["# Exercise 3: DNN versus RF\n","\n","Which method (DNN or Random Forest) has the best classification performance? Are these results in agreement with your expectations? Explain why.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IRDLfaj2oy3E"},"source":["Answer: The method with the best classification performance was the DNN, since it was the one with a higher value of the area under the ROC curve. The value for the DNN was 0.7456 and for the RF was 0.7259. This result is in agreement with the expectations, since the DNN is a more complex model than the RF: since a DNN is highly non-linear, it's expected to better model the data and have a better performance. It has more trainable parameters, which allows it to better adapt to the data. \n","Although the DNN achieves better results, it is important to note that the RF is a simpler model and is easier to train and interpret, taking less computational resources. While this is not really relevant for the amount of data that we are using, at larger scales the training time and resources spent can easily become a bottleneck. The RF is also less prone to overfitting, which can be a problem for the DNN (solved by using early stopping)."]}],"metadata":{"colab":{"provenance":[{"file_id":"1bEDAAj8EQ8P7HlCMuqlqMfUp6SWoVrvL","timestamp":1716314534258}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
